{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can LLMs Do Math?\n",
    "\n",
    "## Your First Hands-On Exploration of Large Language Models\n",
    "\n",
    "Welcome! In this notebook, you'll discover what happens when we ask an LLM to solve math problems. \n",
    "\n",
    "**Guiding Question:** Are LLMs *computing* answers, or *predicting* them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load the Model\n",
    "\n",
    "First, we'll import the FAIR-LLM framework and load a small, fast language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from fairlib.modules.mal.huggingface_adapter import HuggingFaceAdapter\n",
    "from fairlib.core.message import Message\n",
    "\n",
    "# Load your Hugging Face token from .env file\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "if not token:\n",
    "    print(\"Warning: HUGGING_FACE_HUB_TOKEN not found in .env file!\")\n",
    "    print(\"Please follow the README instructions to set up your token.\")\n",
    "else:\n",
    "    print(\"Token loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Language Model\n",
    "\n",
    "We're using a small Dolphin model (only 3B parameters) that can run on your laptop.\n",
    "\n",
    "**Note:** The first time you run this, it will download the model and cache it on your device. This will take a while be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model...\n",
      "🔧 Loading HuggingFace model: cognitivecomputations/Dolphin3.0-Qwen2.5-3b (quantized=False, stream=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Ready to test.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "print(\"Loading language model...\")\n",
    "llm = HuggingFaceAdapter(model_name=\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "print(\"Model loaded! Ready to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Simple Math (Common Problems)\n",
    "\n",
    "Let's start with arithmetic that appears MILLIONS of times in training data.\n",
    "\n",
    "**Hypothesis:** The LLM will succeed because it has \"seen\" these exact problems before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_math(question, correct_answer):\n",
    "    \"\"\"Helper function to test the LLM on a math problem\"\"\"\n",
    "    messages = [Message(role=\"user\", content=question)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"LLM Answer: {response.content}\")\n",
    "    print(f\"Correct Answer: {correct_answer}\")\n",
    "    \n",
    "    # Simple check if the answer contains the right number\n",
    "    if str(correct_answer) in response.content:\n",
    "        print(\"✓ SUCCESS!\")\n",
    "    else:\n",
    "        print(\"✗ FAILED!\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: 2 + 2?\n",
      "LLM Answer: 2 + 2 equals 4.\n",
      "Correct Answer: 4\n",
      "✓ SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2 + 2 equals 4.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 1: Very simple addition\n",
    "test_math(\"2 + 2?\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: 10 × 10?\n",
      "LLM Answer: 10 × 10 equals 100.\n",
      "Correct Answer: 100\n",
      "✓ SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10 × 10 equals 100.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 2: Simple multiplication\n",
    "test_math(\"10 × 10?\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is 5 squared?\n",
      "LLM Answer: 5 squared is 25.\n",
      "Correct Answer: 25\n",
      "✓ SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5 squared is 25.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 3: Basic exponent\n",
    "test_math(\"What is 5 squared?\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 1\n",
    "\n",
    "**Did the LLM succeed on these simple problems?**\n",
    "\n",
    "Think about:\n",
    "- How many times has \"2 + 2 = 4\" appeared on the internet?\n",
    "- Is the LLM *computing* this answer, or *remembering* it from training data?\n",
    "\n",
    "Write your thoughts here:\n",
    "```\n",
    "Your answer:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Complex Math (Uncommon Problems)\n",
    "\n",
    "Now let's try calculations that are UNLIKELY to appear in training data.\n",
    "\n",
    "**Hypothesis:** The LLM will struggle because these specific calculations probably weren't in its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: 237 × 843?\n",
      "LLM Answer: 237 multiplied by 843 equals 200,191.\n",
      "Correct Answer: 199791\n",
      "✗ FAILED!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'237 multiplied by 843 equals 200,191.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 4: Uncommon multiplication\n",
    "test_math(\"237 × 843?\", 199791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: 8347 × 6291?\n",
      "LLM Answer: To calculate 8347 × 6291, you would multiply each digit of the second number by each digit of the first number, starting from the rightmost digit, and then add the results together. Here's how you would do it:\n",
      "\n",
      "```\n",
      "     8347\n",
      "   × 6291\n",
      "   _______\n",
      "     74734   (7 × 1, 7 × 9, 7 × 2, 7 × 6)\n",
      "   156940    (4 × 1, 4 × 9, 4 × 2, 4 × 6)\n",
      "  500820     (3 × 1, 3 × 9, 3 × 2, 3 × 6)\n",
      "+490840      (2 × 1, 2 × 9, 2 × 2, 2 × 6)\n",
      "_______\n",
      "  52856977\n",
      "```\n",
      "\n",
      "So, 8347 × 6291 = 52856977.\n",
      "Correct Answer: 52510977\n",
      "✗ FAILED!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To calculate 8347 × 6291, you would multiply each digit of the second number by each digit of the first number, starting from the rightmost digit, and then add the results together. Here's how you would do it:\\n\\n```\\n     8347\\n   × 6291\\n   _______\\n     74734   (7 × 1, 7 × 9, 7 × 2, 7 × 6)\\n   156940    (4 × 1, 4 × 9, 4 × 2, 4 × 6)\\n  500820     (3 × 1, 3 × 9, 3 × 2, 3 × 6)\\n+490840      (2 × 1, 2 × 9, 2 × 2, 2 × 6)\\n_______\\n  52856977\\n```\\n\\nSo, 8347 × 6291 = 52856977.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 5: Larger uncommon multiplication\n",
    "test_math(\"8347 × 6291?\", 52510977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: (456 + 789) × 234?\n",
      "LLM Answer: (456 + 789) × 234 = 1245 × 234 = 291,530\n",
      "Correct Answer: 291330\n",
      "✗ FAILED!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(456 + 789) × 234 = 1245 × 234 = 291,530'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 6: Multi-step calculation\n",
    "test_math(\"(456 + 789) × 234?\", 291330)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 2\n",
    "\n",
    "**What happened with the complex problems?**\n",
    "\n",
    "Think about:\n",
    "- Did the LLM get the exact right answer?\n",
    "- If it was wrong, was it *close*? Or completely off?\n",
    "- What does this tell you about how the LLM is \"solving\" these problems?\n",
    "\n",
    "Write your thoughts here:\n",
    "```\n",
    "Your answer:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Reveal: What's Really Happening?\n",
    "\n",
    "Let's verify these answers with actual computation (Python's built-in calculator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's verify with REAL computation:\n",
      "\n",
      "2 + 2 = 4\n",
      "10 × 10 = 100\n",
      "5² = 25\n",
      "\n",
      "237 × 843 = 199791\n",
      "8347 × 6291 = 52510977\n",
      "(456 + 789) × 234 = 291330\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's verify with REAL computation:\\n\")\n",
    "print(f\"2 + 2 = {2 + 2}\")\n",
    "print(f\"10 × 10 = {10 * 10}\")\n",
    "print(f\"5² = {5**2}\")\n",
    "print(f\"\\n237 × 843 = {237 * 843}\")\n",
    "print(f\"8347 × 6291 = {8347 * 6291}\")\n",
    "print(f\"(456 + 789) × 234 = {(456 + 789) * 234}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding What's Happening\n",
    "\n",
    "### The LLM's Process:\n",
    "\n",
    "When you ask \"What is 237 × 843?\", the LLM:\n",
    "\n",
    "1. **Tokenizes** the input: `[\"What\", \"is\", \"237\", \"×\", \"843\", \"?\"]`\n",
    "2. **Uses self-attention** to look at all tokens simultaneously\n",
    "3. **Predicts** the most likely next tokens based on patterns it saw during training\n",
    "4. **Generates** a sequence of digits that \"looks like\" an answer\n",
    "\n",
    "### It's NOT:\n",
    "- ❌ Applying multiplication algorithms\n",
    "- ❌ Using a calculator\n",
    "- ❌ Computing the result\n",
    "\n",
    "### It IS:\n",
    "- ✅ Pattern matching from training data\n",
    "- ✅ Predicting \"what numbers usually follow this pattern\"\n",
    "- ✅ Making educated guesses based on statistical patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Let's Break It Further\n",
    "\n",
    "Try asking the LLM to explain its reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM's 'Work':\n",
      "Step 1: Multiply the ones place digits\n",
      "7 × 1 = 7\n",
      "\n",
      "Step 2: Multiply the tens place digits\n",
      "4 × 9 = 36\n",
      "Write down 6 and carry over the 3 to the next step\n",
      "\n",
      "Step 3: Multiply the hundreds place digits\n",
      "3 × 2 = 6\n",
      "Add the carried over 3 to get 9\n",
      "Write down 9\n",
      "\n",
      "Step 4: Multiply the thousands place digits\n",
      "7 × 6 = 42\n",
      "Write down 42\n",
      "\n",
      "Step 5: Multiply the ten-thousands place digits\n",
      "8 × 1 = 8\n",
      "Write down 8\n",
      "\n",
      "Step 6: Multiply the hundred-thousands place digits\n",
      "3 × 2 = 6\n",
      "Write down 6\n",
      "\n",
      "Step 7: Multiply the millions place digits\n",
      "8 × 6 = 48\n",
      "Write down 48\n",
      "\n",
      "Step 8: Add all the products together\n",
      "84827\n",
      "6966\n",
      "------\n",
      "8347\n",
      "--------\n",
      "75206837\n",
      "\n",
      "The final answer is 8347 × 6291 = 52646837\n",
      "\n",
      "Actual answer: 52510977\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    Message(\n",
    "        role=\"user\",\n",
    "        content=\"Solve 8347 × 6291 step by step. Show your work.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(f\"LLM's 'Work':\\n{response.content}\")\n",
    "print(f\"\\nActual answer: {8347 * 6291}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 3\n",
    "\n",
    "**Did the LLM's \"step by step\" work actually compute the right answer?**\n",
    "\n",
    "This reveals something profound: The LLM can generate text that *looks like* mathematical reasoning, but it's not actually performing the computation!\n",
    "\n",
    "Write your thoughts:\n",
    "```\n",
    "Your answer:\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Insight\n",
    "\n",
    "### LLMs Don't Compute - They Predict!\n",
    "\n",
    "**Simple problems:** The exact pattern (`2 + 2 = 4`) appeared millions of times in training data\n",
    "→ LLM *remembers* and succeeds \n",
    "\n",
    "**Complex problems:** The exact pattern (`8347 × 6291 = 52507377`) probably never appeared in training data  \n",
    "→ LLM *guesses* based on \"big number × big number = bigger number\" pattern  \n",
    "→ LLM fails \n",
    "\n",
    "### This is Pattern Matching, Not Calculation\n",
    "\n",
    "The LLM is:\n",
    "- Predicting the next token (number)\n",
    "- Using statistical patterns from training\n",
    "- NOT applying mathematical algorithms\n",
    "- NOT using a calculator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Experiment!\n",
    "\n",
    "Try your own math problems. See if you can find the boundary between \"succeeds\" and \"fails\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is 15 × 23?\n",
      "LLM Answer: 15 × 23 = 345\n",
      "Correct Answer: 345\n",
      "✓ SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'15 × 23 = 345'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try your own math problem here!\n",
    "your_question = \"What is 15 × 23?\"  # Change this to whatever you want\n",
    "your_answer = 15 * 23  # Python computes the real answer\n",
    "\n",
    "test_math(your_question, your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What's Next? Why Do We Need Agents?\n",
    "\n",
    "You've just discovered a fundamental limitation of LLMs:\n",
    "- **They're amazing at language understanding and generation**\n",
    "- **But they can't reliably execute tasks that require computation**\n",
    "\n",
    "### The Solution: LLM Agents\n",
    "\n",
    "What if we could:\n",
    "1. Keep the LLM's language understanding (\"the user wants to multiply two numbers\")\n",
    "2. Give it access to actual TOOLS (a real calculator, Python code, databases, APIs)\n",
    "3. Let it decide WHEN to use tools vs. when to use its own knowledge\n",
    "\n",
    "**That's what we're going to build!**\n",
    "\n",
    "### Agent Architecture:\n",
    "```\n",
    "User: \"What's 8347 × 6291?\"\n",
    "  ↓\n",
    "LLM: \"I need to multiply. I should use the calculator tool.\"\n",
    "  ↓\n",
    "Tool: calculator(8347, 6291) → 52507377\n",
    "  ↓\n",
    "LLM: \"The answer is 52,507,377\"\n",
    "  ↓\n",
    "User: Correct!\n",
    "```\n",
    "\n",
    "**LLM = Understanding**  \n",
    "**Tools = Execution**  \n",
    "**Agent = LLM + Tools = Actually useful!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your First Agent: Calculator Edition\n",
    "\n",
    "Now let's put everything together! We'll build an agent that:\n",
    "1. **Understands** your math question (LLM brain)\n",
    "2. **Decides** when to use a calculator (reasoning)\n",
    "3. **Executes** the calculation (tool use)\n",
    "4. **Responds** with the correct answer\n",
    "\n",
    "This is the bridge from \"predicting text\" to \"actually computing\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the FAIR-LLM Components\n",
    "\n",
    "We need to import the building blocks for our agent:\n",
    "- **HuggingFaceAdapter**: The LLM \"brain\"\n",
    "- **ToolRegistry**: Holds all available tools\n",
    "- **SafeCalculatorTool**: A calculator the agent can use\n",
    "- **ToolExecutor**: Actually runs the tools\n",
    "- **WorkingMemory**: Keeps track of the conversation\n",
    "- **SimpleAgent**: Brings everything together\n",
    "- **SimpleReActPlanner**: The reasoning engine (ReAct = Reason + Act)\n",
    "- **RoleDefinition**: Defines the agent's persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All components imported!\n"
     ]
    }
   ],
   "source": [
    "from fairlib import (\n",
    "    HuggingFaceAdapter,\n",
    "    ToolRegistry,\n",
    "    SafeCalculatorTool,\n",
    "    ToolExecutor,\n",
    "    WorkingMemory,\n",
    "    SimpleAgent, \n",
    "    SimpleReActPlanner,\n",
    "    RoleDefinition\n",
    ")\n",
    "\n",
    "print(\"All components imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Agent's \"Brain\" (LLM)\n",
    "\n",
    "We'll reuse the same LLM from earlier - but now instead of asking it to do math directly, we'll teach it to USE TOOLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading HuggingFace model: cognitivecomputations/Dolphin3.0-Qwen2.5-3b (quantized=False, stream=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent brain created!\n"
     ]
    }
   ],
   "source": [
    "# The same LLM, but now it will be part of an agent system\n",
    "agent_llm = HuggingFaceAdapter(\n",
    "    model_name=\"dolphin3-qwen25-3b\",\n",
    "    auth_token=token,\n",
    "    max_new_tokens=150,  # Longer responses for reasoning\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Agent brain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Give the Agent a Tool (Calculator)\n",
    "\n",
    "This is the KEY difference! Instead of making the LLM do math by prediction, we give it access to a REAL calculator tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering tool: safe_calculator\n",
      "Agent's tools: ['safe_calculator']\n"
     ]
    }
   ],
   "source": [
    "# Create a registry to hold all tools\n",
    "tool_registry = ToolRegistry()\n",
    "\n",
    "# Create the calculator tool\n",
    "calculator_tool = SafeCalculatorTool()\n",
    "\n",
    "# Register it so the agent knows it exists\n",
    "tool_registry.register_tool(calculator_tool)\n",
    "\n",
    "# See what tools are available\n",
    "available_tools = [tool.name for tool in tool_registry.get_all_tools().values()]\n",
    "print(f\"Agent's tools: {available_tools}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Tool Executor\n",
    "\n",
    "The executor is like the agent's \"hands\" - it actually runs the tools when the agent decides to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool executor created!\n"
     ]
    }
   ],
   "source": [
    "# The executor can run any tool in the registry\n",
    "executor = ToolExecutor(tool_registry)\n",
    "\n",
    "print(\"Tool executor created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Give the Agent Memory\n",
    "\n",
    "The agent needs memory to keep track of the conversation and what it has already tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory system created!\n"
     ]
    }
   ],
   "source": [
    "# Simple working memory for the conversation\n",
    "memory = WorkingMemory()\n",
    "\n",
    "print(\"Memory system created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the Planner (The Reasoning Engine)\n",
    "\n",
    "This is where the magic happens! The planner uses the **ReAct** pattern:\n",
    "- **Reason**: Think about what to do next\n",
    "- **Act**: Take an action (use a tool or respond)\n",
    "\n",
    "The planner looks at the conversation, decides if it needs to use a tool, and calls it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planner created with detailed role definition!\n"
     ]
    }
   ],
   "source": [
    "# Create the planner\n",
    "planner = SimpleReActPlanner(agent_llm, tool_registry)\n",
    "\n",
    "# CRITICAL: Properly orient the agent with detailed instructions\n",
    "planner.prompt_builder.role_definition = RoleDefinition(\n",
    "    \"You are an expert mathematical calculator. Your job is to perform mathematical calculations. \"\n",
    "    \"You reason step-by-step to determine the best course of action. \"\n",
    "    \"When you receive a calculation request, you MUST use the calculator tool - do not attempt to calculate in your head. \"\n",
    "    \"If a user's request requires multiple steps or tools, you must break it down and execute them sequentially. \"\n",
    "    \"You must follow the strict formatting rules for tool calling. \"\n",
    "    \"Always use tools for calculations to ensure accuracy.\"\n",
    ")\n",
    "\n",
    "print(\"Planner created with detailed role definition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Assemble the Complete Agent\n",
    "\n",
    "Now we bring all the pieces together into a functioning agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent successfully created!\n",
      "The agent can now:\n",
      "  Understand your math questions\n",
      "  Decide when to use the calculator\n",
      "  Execute calculations accurately\n",
      "  Respond with correct answers\n"
     ]
    }
   ],
   "source": [
    "# Create the agent with all components\n",
    "agent = SimpleAgent(\n",
    "    llm=agent_llm,\n",
    "    planner=planner,\n",
    "    tool_executor=executor,\n",
    "    memory=memory,\n",
    "    max_steps=10  # Limit to prevent infinite loops\n",
    ")\n",
    "\n",
    "print(\"Agent successfully created!\")\n",
    "print(\"The agent can now:\")\n",
    "print(\"  Understand your math questions\")\n",
    "print(\"  Decide when to use the calculator\")\n",
    "print(\"  Execute calculations accurately\")\n",
    "print(\"  Respond with correct answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent\n",
    "\n",
    "Let's test our agent on the SAME problems that the pure LLM failed on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for the agent\n",
    "async def test_agent_math(question):\n",
    "    \"\"\"Test the agent on a math problem\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"Agent thinking...\")\n",
    "    \n",
    "    response = await agent.arun(question)\n",
    "    print(f\"Agent: {response}\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Simple Math (This Should Still Work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is 2 + 2?\n",
      "Agent thinking...\n",
      "--- Step 1/10 ---\n"
     ]
    }
   ],
   "source": [
    "# Test on simple problem\n",
    "await test_agent_math(\"What is 2 + 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Complex Math (The LLM Failed This - Will the Agent Succeed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem the LLM got wrong!\n",
    "await test_agent_math(\"What is 8347 × 6291?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Multi-Step Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_agent_math(\"What is (456 + 789) × 234?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 The Key Insight: What Changed?\n",
    "\n",
    "### Pure LLM (Earlier):\n",
    "```\n",
    "User: \"What is 8347 × 6291?\"\n",
    "  ↓\n",
    "LLM: *predicts next tokens based on patterns*\n",
    "  ↓\n",
    "LLM: \"52,487,000\" (wrong - it guessed!)\n",
    "```\n",
    "\n",
    "### LLM Agent (Now):\n",
    "```\n",
    "User: \"What is 8347 × 6291?\"\n",
    "  ↓\n",
    "Agent: *reasons* \"I need to calculate. I should use the calculator tool.\"\n",
    "  ↓\n",
    "Agent: *calls* calculator(8347, 6291)\n",
    "  ↓\n",
    "Calculator: 52,507,377\n",
    "  ↓\n",
    "Agent: \"The answer is 52,507,377\"\n",
    "```\n",
    "\n",
    "**The LLM didn't get smarter at math - we gave it the RIGHT TOOL for the job!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "Answer these questions based on what you just saw:\n",
    "\n",
    "### 1. What's the difference between the pure LLM and the agent?\n",
    "```\n",
    "Your answer:\n",
    "```\n",
    "\n",
    "### 2. Why can the agent do math reliably while the pure LLM can't?\n",
    "```\n",
    "Your answer:\n",
    "```\n",
    "\n",
    "### 3. What other tools could you give an agent to make it more capable?\n",
    "```\n",
    "Your answer:\n",
    "```\n",
    "\n",
    "### 4. Can you think of a task where tool use is essential?\n",
    "```\n",
    "Your answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've just built your first LLM agent! You've learned:\n",
    "\n",
    "**LLMs predict text** - they don't compute  \n",
    "**Agents = LLM + Tools** - extend capabilities beyond prediction  \n",
    "**ReAct pattern** - Reason about what to do, then Act  \n",
    "**Tool calling** - Agents can use external functions/APIs  \n",
    "**Reliable execution** - Tools provide accuracy that prediction can't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Math Demo)",
   "language": "python",
   "name": "llm-math-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
