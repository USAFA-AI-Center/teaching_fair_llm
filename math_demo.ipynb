{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can LLMs Do Math?\n",
    "\n",
    "## Your First Hands-On Exploration of Large Language Models\n",
    "\n",
    "Welcome! In this notebook, you'll discover what happens when we ask an LLM to solve math problems. \n",
    "\n",
    "**Guiding Question:** Are LLMs *computing* answers, or *predicting* them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Load the Model\n",
    "\n",
    "First, we'll import the FAIR-LLM framework and load a small, fast language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from fairlib.modules.mal.huggingface_adapter import HuggingFaceAdapter\n",
    "from fairlib.core.message import Message\n",
    "\n",
    "# Load your Hugging Face token from .env file\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "if not token:\n",
    "    print(\"Warning: HUGGING_FACE_HUB_TOKEN not found in .env file!\")\n",
    "    print(\"Please follow the README instructions to set up your token.\")\n",
    "else:\n",
    "    print(\"Token loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Language Model\n",
    "\n",
    "We're using a small Dolphin model (only 3B parameters) that can run on your laptop.\n",
    "\n",
    "**Note:** The first time you run this, it will download the model and cache it on your device. This will take a while be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "print(\"Loading language model...\")\n",
    "llm = HuggingFaceAdapter(model_name=\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "print(\"Model loaded! Ready to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Simple Math (Common Problems)\n",
    "\n",
    "Let's start with arithmetic that appears MILLIONS of times in training data.\n",
    "\n",
    "**Hypothesis:** The LLM will succeed because it has \"seen\" these exact problems before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_math(question, correct_answer):\n",
    "    \"\"\"Helper function to test the LLM on a math problem\"\"\"\n",
    "    messages = [Message(role=\"user\", content=question)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"LLM Answer: {response.content}\")\n",
    "    print(f\"Correct Answer: {correct_answer}\")\n",
    "    \n",
    "    # Simple check if the answer contains the right number\n",
    "    if str(correct_answer) in response.content:\n",
    "        print(\"✓ SUCCESS!\")\n",
    "    else:\n",
    "        print(\"✗ FAILED!\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Very simple addition\n",
    "test_math(\"2 + 2?\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Simple multiplication\n",
    "test_math(\"10 × 10?\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Basic exponent\n",
    "test_math(\"What is 5 squared?\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 1\n",
    "\n",
    "**Did the LLM succeed on these simple problems?**\n",
    "\n",
    "Think about:\n",
    "- How many times has \"2 + 2 = 4\" appeared on the internet?\n",
    "- Is the LLM *computing* this answer, or *remembering* it from training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Complex Math (Uncommon Problems)\n",
    "\n",
    "Now let's try calculations that are UNLIKELY to appear in training data.\n",
    "\n",
    "**Hypothesis:** The LLM will struggle because these specific calculations probably weren't in its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Uncommon multiplication\n",
    "test_math(\"237 × 843?\", 199791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Larger uncommon multiplication\n",
    "test_math(\"8347 × 6291?\", 52510977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Multi-step calculation\n",
    "test_math(\"(456 + 789) × 234?\", 291330)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 2\n",
    "\n",
    "**What happened with the complex problems?**\n",
    "\n",
    "Think about:\n",
    "- Did the LLM get the exact right answer?\n",
    "- If it was wrong, was it *close*? Or completely off?\n",
    "- What does this tell you about how the LLM is \"solving\" these problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding What's Happening\n",
    "\n",
    "### The LLM's Process:\n",
    "\n",
    "When you ask \"What is 237 × 843?\", the LLM:\n",
    "\n",
    "1. **Tokenizes** the input: `[\"What\", \"is\", \"237\", \"×\", \"843\", \"?\"]`\n",
    "2. **Uses self-attention** to look at all tokens simultaneously\n",
    "3. **Predicts** the most likely next tokens based on patterns it saw during training\n",
    "4. **Generates** a sequence of digits that \"looks like\" an answer\n",
    "\n",
    "### It's NOT:\n",
    "- ❌ Applying multiplication algorithms\n",
    "- ❌ Using a calculator\n",
    "- ❌ Computing the result\n",
    "\n",
    "### It IS:\n",
    "- ✅ Pattern matching from training data\n",
    "- ✅ Predicting \"what numbers usually follow this pattern\"\n",
    "- ✅ Making educated guesses based on statistical patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Let's Break It Further\n",
    "\n",
    "Try asking the LLM to explain its reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    Message(\n",
    "        role=\"user\",\n",
    "        content=\"Solve 8347 × 6291 step by step. Show your work.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(f\"LLM's 'Work':\\n{response.content}\")\n",
    "print(f\"\\nActual answer: {8347 * 6291}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 3\n",
    "\n",
    "**Did the LLM's \"step by step\" work actually compute the right answer?**\n",
    "\n",
    "This reveals something profound: The LLM can generate text that *looks like* mathematical reasoning, but it's not actually performing the computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Insight\n",
    "\n",
    "### LLMs Don't Compute - They Predict!\n",
    "\n",
    "**Simple problems:** The exact pattern (`2 + 2 = 4`) appeared millions of times in training data\n",
    "→ LLM *remembers* and succeeds \n",
    "\n",
    "**Complex problems:** The exact pattern (`8347 × 6291 = 52507377`) probably never appeared in training data  \n",
    "→ LLM *guesses* based on \"big number × big number = bigger number\" pattern  \n",
    "→ LLM fails \n",
    "\n",
    "### This is Pattern Matching, Not Calculation\n",
    "\n",
    "The LLM is:\n",
    "- Predicting the next token (number)\n",
    "- Using statistical patterns from training\n",
    "- NOT applying mathematical algorithms\n",
    "- NOT using a calculator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Experiment!\n",
    "\n",
    "Try your own math problems. See if you can find the boundary between \"succeeds\" and \"fails\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_question = \"What is 15 × 23?\"  # Change this to whatever you want\n",
    "your_answer = 15 * 23\n",
    "\n",
    "test_math(your_question, your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What's Next? Why Do We Need Agents?\n",
    "\n",
    "You've just discovered a fundamental limitation of LLMs:\n",
    "- **They're amazing at language understanding and generation**\n",
    "- **But they can't reliably execute tasks that require computation**\n",
    "\n",
    "### The Solution: LLM Agents\n",
    "\n",
    "What if we could:\n",
    "1. Keep the LLM's language understanding (\"the user wants to multiply two numbers\")\n",
    "2. Give it access to actual TOOLS (a real calculator, Python code, databases, APIs)\n",
    "3. Let it decide WHEN to use tools vs. when to use its own knowledge\n",
    "\n",
    "**That's what we're going to build!**\n",
    "\n",
    "### Agent Architecture:\n",
    "```\n",
    "User: \"What's 8347 × 6291?\"\n",
    "  ↓\n",
    "LLM: \"I need to multiply. I should use the calculator tool.\"\n",
    "  ↓\n",
    "Tool: calculator(8347, 6291) → 52507377\n",
    "  ↓\n",
    "LLM: \"The answer is 52,507,377\"\n",
    "  ↓\n",
    "User: Correct!\n",
    "```\n",
    "\n",
    "**LLM = Understanding**  \n",
    "**Tools = Execution**  \n",
    "**Agent = LLM + Tools = Actually useful!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Your First Agent: Calculator Edition\n",
    "\n",
    "Now let's put everything together! We'll build an agent that:\n",
    "1. **Understands** your math question (LLM brain)\n",
    "2. **Decides** when to use a calculator (reasoning)\n",
    "3. **Executes** the calculation (tool use)\n",
    "4. **Responds** with the correct answer\n",
    "\n",
    "This is the bridge from \"predicting text\" to \"actually computing\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the FAIR-LLM Components\n",
    "\n",
    "We need to import the building blocks for our agent:\n",
    "- **HuggingFaceAdapter**: The LLM \"brain\"\n",
    "- **ToolRegistry**: Holds all available tools\n",
    "- **SafeCalculatorTool**: A calculator the agent can use\n",
    "- **ToolExecutor**: Actually runs the tools\n",
    "- **WorkingMemory**: Keeps track of the conversation\n",
    "- **SimpleAgent**: Brings everything together\n",
    "- **SimpleReActPlanner**: The reasoning engine (ReAct = Reason + Act)\n",
    "- **RoleDefinition**: Defines the agent's persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlib import (\n",
    "    HuggingFaceAdapter,\n",
    "    ToolRegistry,\n",
    "    SafeCalculatorTool,\n",
    "    ToolExecutor,\n",
    "    WorkingMemory,\n",
    "    SimpleAgent, \n",
    "    SimpleReActPlanner,\n",
    "    RoleDefinition\n",
    ")\n",
    "\n",
    "print(\"All components imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Agent's \"Brain\" (LLM)\n",
    "\n",
    "We'll reuse the same LLM from earlier - but now instead of asking it to do math directly, we'll TEACH IT to USE TOOLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same LLM, but now it will be part of an agent system\n",
    "agent_llm = HuggingFaceAdapter(model_name=\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "print(\"Agent brain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Give the Agent a Tool (Calculator)\n",
    "\n",
    "This is the KEY difference! Instead of making the LLM do math by prediction, we give it access to a REAL calculator tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a registry to hold all tools\n",
    "tool_registry = ToolRegistry()\n",
    "\n",
    "# Create the calculator tool\n",
    "calculator_tool = SafeCalculatorTool()\n",
    "\n",
    "# Register it so the agent knows it exists\n",
    "tool_registry.register_tool(calculator_tool)\n",
    "\n",
    "# See what tools are available\n",
    "available_tools = [tool.name for tool in tool_registry.get_all_tools().values()]\n",
    "print(f\"Agent's tools: {available_tools}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Tool Executor\n",
    "\n",
    "The executor is like the agent's \"hands\" - it actually runs the tools when the agent decides to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The executor can run any tool in the registry\n",
    "executor = ToolExecutor(tool_registry)\n",
    "\n",
    "print(\"Tool executor created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Give the Agent Memory\n",
    "\n",
    "The agent needs memory to keep track of the conversation and what it has already tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple working memory for the conversation\n",
    "memory = WorkingMemory()\n",
    "\n",
    "print(\"Memory system created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the Planner (The Reasoning Engine)\n",
    "\n",
    "This is where the magic happens! The planner uses the **ReAct** pattern:\n",
    "- **Reason**: Think about what to do next\n",
    "- **Act**: Take an action (use a tool or respond)\n",
    "\n",
    "The planner looks at the conversation, decides if it needs to use a tool, and calls it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the planner\n",
    "planner = SimpleReActPlanner(agent_llm, tool_registry)\n",
    "\n",
    "# CRITICAL: Properly orient the agent with detailed instructions\n",
    "planner.prompt_builder.role_definition = RoleDefinition(\n",
    "    \"You are an expert mathematical calculator. Your job is to perform mathematical calculations. \"\n",
    "    \"You reason step-by-step to determine the best course of action. \"\n",
    "    \"When you receive a calculation request, you MUST use the calculator tool - do not attempt to calculate in your head. \"\n",
    "    \"If a user's request requires multiple steps, you must break it down and execute them sequentially. \"\n",
    "    \"You must follow the strict formatting rules for tool calling. \"\n",
    "    \"Always use tools for calculations to ensure accuracy.\"\n",
    ")\n",
    "\n",
    "print(\"Planner created with detailed role definition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Assemble the Complete Agent\n",
    "\n",
    "Now we bring all the pieces together into a functioning agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent with all components\n",
    "agent = SimpleAgent(\n",
    "    llm=agent_llm,\n",
    "    planner=planner,\n",
    "    tool_executor=executor,\n",
    "    memory=memory,\n",
    "    max_steps=10\n",
    ")\n",
    "\n",
    "print(\"Agent successfully created!\")\n",
    "print(\"The agent can now:\")\n",
    "print(\"  Understand your math questions\")\n",
    "print(\"  Decide when to use the calculator\")\n",
    "print(\"  Execute calculations accurately\")\n",
    "print(\"  Respond with correct answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent\n",
    "\n",
    "Let's test our agent on the SAME problems that the pure LLM failed on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for the agent\n",
    "async def test_agent_math(question):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"Agent thinking...\")\n",
    "    \n",
    "    response = await agent.arun(question) # this line invokes the entire agent reasoning loop!\n",
    "    \n",
    "    print(f\"Agent: {response}\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Simple Math (This Should Still Work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on simple problem\n",
    "await test_agent_math(\"What is 2 + 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Complex Math (The LLM Failed This - Will the Agent Succeed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem the LLM got wrong!\n",
    "await test_agent_math(\"What is 8347 × 6291?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Multi-Step Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_agent_math(\"What is (456 + 789) × 234?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 The Key Insight: What Changed?\n",
    "\n",
    "### Pure LLM (Earlier):\n",
    "```\n",
    "User: \"What is 8347 × 6291?\"\n",
    "  ↓\n",
    "LLM: [predicts next tokens based on patterns]\n",
    "  ↓\n",
    "LLM: \"52,487,000\" (wrong - it guessed!)\n",
    "```\n",
    "\n",
    "### LLM Agent (Now):\n",
    "```\n",
    "User: \"What is 8347 × 6291?\"\n",
    "  ↓\n",
    "Agent: [reasons] \"I need to calculate. I should use the calculator tool.\"\n",
    "  ↓\n",
    "Agent: [calls] calculator(8347, 6291)\n",
    "  ↓\n",
    "Calculator: 52,507,377\n",
    "  ↓\n",
    "Agent: \"The answer is 52,507,377\"\n",
    "```\n",
    "\n",
    "**The LLM didn't get better at math - we gave it the RIGHT TOOL for the job!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "Take a second to think about the following questions.\n",
    "\n",
    "### 1. What's the difference between the pure LLM and the agent?\n",
    "### 2. Why can the agent do math reliably while the pure LLM can't?\n",
    "### 3. What other tools could you give an agent to make it more capable?\n",
    "### 4. Can you think of a task where tool use is essential?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've just built your first LLM agent! You've learned:\n",
    "\n",
    "**LLMs predict text** - they don't compute  \n",
    "**Agents = LLM + Tools** - extend capabilities beyond prediction  \n",
    "**ReAct pattern** - Reason about what to do, then Act  \n",
    "**Tool calling** - Agents can use external functions/APIs  \n",
    "**Reliable execution** - Tools provide accuracy that prediction can't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Time\n",
    "\n",
    "We will learn to create a commitee of agents to solve complex tasks. \n",
    "\n",
    "We will learn how to ground language models using our data for more accurate query responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Math Demo)",
   "language": "python",
   "name": "llm-math-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
