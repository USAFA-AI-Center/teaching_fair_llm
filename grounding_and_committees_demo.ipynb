{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Advanced Agentic AI\n",
    "\n",
    "## Grounding Foundation Models & Multi-Agent Committees\n",
    "\n",
    "In Lesson 1, you learned that LLMs are powerful pattern predictors but have limitations:\n",
    "- They can't access real-time information\n",
    "- They don't know about YOUR specific data\n",
    "- They work in isolation\n",
    "\n",
    "Today, we'll solve these problems by:\n",
    "1. **Grounding** LLMs with your own knowledge base\n",
    "2. **Coordinating** multiple agents to solve complex tasks\n",
    "\n",
    "# **Guiding Questions:**\n",
    "1. How can we make LLMs answer questions about information they've never seen?\n",
    "2. Can multiple AI agents work together better than one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Grounding Foundation Models\n",
    "\n",
    "## What Does \"Grounding\" Mean?\n",
    "\n",
    "**Grounding** means connecting an LLM's responses to specific, verifiable sources of information.\n",
    "\n",
    "### The Problem:\n",
    "- LLMs only know what was in their training data (cutoff: early 2024 for most models)\n",
    "- They hallucinate when they don't know something\n",
    "- They can't access YOUR documents, databases, or private information\n",
    "\n",
    "### The Solution: RAG (Retrieval-Augmented Generation)\n",
    "**RAG** = Retrieve relevant information → Augment the prompt → Generate grounded responses\n",
    "\n",
    "Think of it like an open-book exam vs. a closed-book exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "Let's set up our environment for building a grounded agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from fairlib.utils.document_processor import DocumentProcessor\n",
    "\n",
    "from fairlib import (\n",
    "    settings,\n",
    "    AbstractTool,\n",
    "    ManagerPlanner,\n",
    "    HierarchicalAgentRunner,\n",
    "    Message,\n",
    "    HuggingFaceAdapter,\n",
    "    ToolRegistry,\n",
    "    ToolExecutor,\n",
    "    WorkingMemory,\n",
    "    LongTermMemory,\n",
    "    ChromaDBVectorStore,\n",
    "    ReActPlanner,\n",
    "    SimpleAgent,\n",
    "    SentenceTransformerEmbedder,\n",
    "    SimpleRetriever,\n",
    "    KnowledgeBaseQueryTool  # <-- Using the official framework tool\n",
    ")\n",
    "\n",
    "# ChromaDB for vector storage\n",
    "try:\n",
    "    import chromadb\n",
    "    CHROMADB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: chromadb not installed. Install with: pip install chromadb\")\n",
    "    CHROMADB_AVAILABLE = False\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "if not token:\n",
    "    print(\"Warning: HUGGING_FACE_HUB_TOKEN not found in .env file!\")\n",
    "else:\n",
    "    print(\"Token loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: The Hallucination Problem\n",
    "\n",
    "Let's see what happens when we ask an LLM about information it doesn't have.\n",
    "\n",
    "**Try this**: Google \"UCCS Kraemer Library hours\" and see what the actual hours are. Then compare to what the LLM says!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a language model\n",
    "print(\"Loading language model...\")\n",
    "llm = HuggingFaceAdapter(\n",
    "    model_name=\"dolphin3-qwen25-3b\", \n",
    "    auth_token=token,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about UCCS library hours\n",
    "question = \"What time does the Kraemer Family Library at UCCS close on Friday nights?\"\n",
    "\n",
    "messages = [Message(role=\"user\", content=question)]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nLLM Response:\\n{response.content}\")\n",
    "print(\"\\nWARNING: This information might be completely made up!\")\n",
    "print(\"\\nGoogle 'What time does the Kraemer Family Library at UCCS close on Friday nights?' to verify if this is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 1\n",
    "\n",
    "**Did the LLM give you the right answer? Did it admit it doesn't know, or did it make something up?**\n",
    "\n",
    "This is the hallucination problem: LLMs try to answer even when they don't have the information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Knowledge Base with Real Documents\n",
    "\n",
    "Instead of relying on the LLM's training data, let's create actual documents with UCCS information.\n",
    "\n",
    "In a real system, you'd have PDFs, Word docs, or web pages. For this demo, we'll create text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for our knowledge base documents\n",
    "kb_directory = \"./uccs_knowledge_base\"\n",
    "os.makedirs(kb_directory, exist_ok=True)\n",
    "\n",
    "# Document 1: Library Hours\n",
    "library_hours_doc = \"\"\"Kraemer Family Library Hours (UCCS)\n",
    "\n",
    "Fall/Spring Semester Regular Hours:\n",
    "- Monday - Thursday: 7:30 AM - 11:00 PM\n",
    "- Friday: 7:30 AM - 6:00 PM\n",
    "- Saturday: 10:00 AM - 6:00 PM\n",
    "- Sunday: 12:00 PM - 11:00 PM\n",
    "\n",
    "Finals Week Extended Hours:\n",
    "- Sunday - Thursday: 7:30 AM - 2:00 AM\n",
    "- Friday: 7:30 AM - 6:00 PM\n",
    "- Saturday: 10:00 AM - 6:00 PM\n",
    "\n",
    "Summer and Break Hours:\n",
    "- Monday - Friday: 8:00 AM - 5:00 PM\n",
    "- Closed on weekends\n",
    "\n",
    "Note: Hours may vary during holidays. Always check library.uccs.edu for the most current information.\n",
    "\"\"\"\n",
    "\n",
    "# Document 2: Degree Requirements\n",
    "degree_requirements_doc = \"\"\"UCCS Computer Science B.S. Degree Requirements\n",
    "\n",
    "Total Credit Hours Required: 120\n",
    "\n",
    "Core Computer Science Courses: 48 credit hours\n",
    "- CS 1030, 1050, 1150, 1200, 2060, 2100, 2400\n",
    "- CS 3100, 3150, 3300, 3400, 3500, 4200\n",
    "- CS 4310 (Senior Design I) and CS 4320 (Senior Design II)\n",
    "\n",
    "Mathematics Requirements: 16 credit hours\n",
    "- MATH 1310, 1320, 2130, 2420\n",
    "\n",
    "General Education: 35 credit hours\n",
    "- UCCS Core and Compass Curriculum requirements\n",
    "\n",
    "Electives and Free Choice: 21 credit hours\n",
    "\n",
    "Additional Requirements:\n",
    "- Minimum 2.0 GPA in major courses\n",
    "- At least 30 credit hours must be upper-division (3000-4000 level)\n",
    "- Senior design capstone project required\n",
    "- Complete at least 45 credit hours at UCCS\n",
    "\"\"\"\n",
    "\n",
    "# Document 3: Campus Resources\n",
    "campus_resources_doc = \"\"\"UCCS Student Resources and Services\n",
    "\n",
    "Writing Center:\n",
    "- Location: Columbine Hall, Room 108\n",
    "- Services: Free writing tutoring for all UCCS students\n",
    "- Walk-in hours: Monday-Friday, 9:00 AM - 5:00 PM\n",
    "- Online appointments available via Zoom\n",
    "- Website: writingcenter.uccs.edu\n",
    "\n",
    "Wellness Center:\n",
    "- Location: University Center, Second Floor\n",
    "- Services: Medical care, counseling, health education\n",
    "- Phone: 719-255-4444\n",
    "- Hours: Monday-Friday, 8:00 AM - 5:00 PM\n",
    "- Crisis support available 24/7\n",
    "\n",
    "Career Center:\n",
    "- Location: Cragmor Hall, Room 111\n",
    "- Services: Resume reviews, interview preparation, job search assistance\n",
    "- Handshake platform for job/internship postings\n",
    "- Career fairs held each semester (Fall and Spring)\n",
    "- One-on-one advising appointments available\n",
    "\n",
    "Math Learning Center:\n",
    "- Location: Engineering Building, Room 105\n",
    "- Free tutoring for math courses up to Calculus II\n",
    "- Walk-in hours: Monday-Thursday, 10:00 AM - 6:00 PM\n",
    "\n",
    "Computer Science Tutoring Lab:\n",
    "- Location: Engineering Building, Room 207\n",
    "- Free tutoring for CS 1030, 1050, 1150, 1200, 2060\n",
    "- Hours: Monday-Friday, 12:00 PM - 5:00 PM\n",
    "\"\"\"\n",
    "\n",
    "# Save documents to files\n",
    "with open(os.path.join(kb_directory, \"library_hours.txt\"), \"w\") as f:\n",
    "    f.write(library_hours_doc)\n",
    "\n",
    "with open(os.path.join(kb_directory, \"degree_requirements.txt\"), \"w\") as f:\n",
    "    f.write(degree_requirements_doc)\n",
    "\n",
    "with open(os.path.join(kb_directory, \"campus_resources.txt\"), \"w\") as f:\n",
    "    f.write(campus_resources_doc)\n",
    "\n",
    "print(\"Knowledge base documents created!\")\n",
    "print(f\"Saved to: {kb_directory}\")\n",
    "print(f\"Documents: library_hours.txt, degree_requirements.txt, campus_resources.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents with FAIR_LLM\n",
    "\n",
    "Now we'll use FAIR_LLM's `DocumentProcessor` to load and chunk these documents.\n",
    "\n",
    "This is the same process you'd use for PDFs, Word docs, PowerPoints, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document processor\n",
    "doc_processor = DocumentProcessor(config={\n",
    "    \"files_directory\": kb_directory,\n",
    "    \"max_chunk_chars\": 1000,\n",
    "    \"supported_extensions\": {\".txt\", \".pdf\", \".docx\"}\n",
    "})\n",
    "\n",
    "print(\"Document processor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents from the folder\n",
    "print(\"Loading and chunking documents...\\n\")\n",
    "documents = doc_processor.load_documents_from_folder()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} document chunks\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"Content: {documents[0].page_content[:200]}...\")\n",
    "print(f\"\\nMetadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG Architecture\n",
    "\n",
    "Now we'll build the proper RAG pipeline with the core FAIR_LLM components:\n",
    "\n",
    "1. **Embedder**: Converts text into numerical vectors\n",
    "2. **Vector Store**: Stores embeddings and enables similarity search  \n",
    "3. **Long-Term Memory**: Wraps the vector store\n",
    "4. **Retriever**: Queries the vector store to find relevant documents\n",
    "\n",
    "**How it works:**\n",
    "1. Documents are converted into vector embeddings (numerical representations)\n",
    "2. Embeddings are stored in a vector database (ChromaDB)\n",
    "3. When you query, the query is converted to a vector\n",
    "4. Vector store finds the most similar document vectors (semantic search)\n",
    "5. Retriever returns the most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ChromaDB is available\n",
    "if not CHROMADB_AVAILABLE:\n",
    "    print(\"ERROR: ChromaDB is required for this section.\")\n",
    "    print(\"Install it with: pip install chromadb\")\n",
    "    print(\"\\nSkipping RAG setup...\")\n",
    "else:\n",
    "    print(\"ChromaDB available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the embedder\n",
    "print(\"Step 1: Creating embedder...\")\n",
    "embedder = SentenceTransformerEmbedder(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # Fast, efficient model\n",
    ")\n",
    "print(\"✓ Embedder created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 & 3: Create vector store and add documents (combined to prevent errors)\n",
    "print(\"\\nStep 2 & 3: Creating vector store and adding documents...\")\n",
    "\n",
    "# Create a fresh ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Delete collection if it exists (prevents duplicate IDs on reruns)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"uccs_knowledge_base\")\n",
    "    print(\"  (Cleared existing collection)\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create fresh vector store\n",
    "vector_store = ChromaDBVectorStore(\n",
    "    embedder=embedder,\n",
    "    client=chroma_client,\n",
    "    collection_name=\"uccs_knowledge_base\"\n",
    ")\n",
    "print(\"✓ Vector store created\")\n",
    "\n",
    "# Extract text content from Document objects\n",
    "document_texts = [doc.page_content for doc in documents]\n",
    "\n",
    "# CRITICAL: Remove duplicate documents (ChromaDB uses hash(doc) as ID internally)\n",
    "unique_texts = []\n",
    "seen = set()\n",
    "for text in document_texts:\n",
    "    if text not in seen:\n",
    "        seen.add(text)\n",
    "        unique_texts.append(text)\n",
    "\n",
    "print(f\"  Total chunks: {len(document_texts)}, Unique chunks: {len(unique_texts)}\")\n",
    "\n",
    "# Add to vector store (no ids parameter - ChromaDB generates them internally)\n",
    "vector_store.add_documents(unique_texts)\n",
    "\n",
    "print(f\"✓ Added {len(unique_texts)} document chunks to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Long-Term Memory (wraps the vector store)\n",
    "print(\"\\nStep 4: Creating long-term memory...\")\n",
    "long_term_memory = LongTermMemory(vector_store)\n",
    "print(\"✓ Long-term memory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create the retriever\n",
    "print(\"\\nStep 5: Creating retriever...\")\n",
    "retriever = SimpleRetriever(vector_store)\n",
    "print(\"✓ Retriever created and ready!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Retriever Directly\n",
    "\n",
    "Let's see how semantic search works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "test_query = \"library hours friday\"\n",
    "results = retriever.retrieve(query=test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nTop {len(results)} most relevant chunks:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    # Handle both string and Document results\n",
    "    if isinstance(result, str):\n",
    "        print(f\"Content: {result[:300]}...\\n\")\n",
    "    else:  # Document object\n",
    "        if hasattr(result, 'metadata'):\n",
    "            print(f\"Source: {result.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"Content: {result.page_content[:300] if hasattr(result, 'page_content') else str(result)[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Grounded Agent with RAG\n",
    "\n",
    "Now we'll create an agent that uses the `KnowledgeBaseQueryTool` to ground its responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate LLM instance for the agent\n",
    "agent_llm = llm\n",
    "\n",
    "# Create the knowledge base query tool\n",
    "knowledge_tool = KnowledgeBaseQueryTool(retriever=retriever)\n",
    "\n",
    "# Register the tool\n",
    "tool_registry = ToolRegistry()\n",
    "tool_registry.register_tool(knowledge_tool)\n",
    "\n",
    "# Create executor, memory, and planner\n",
    "executor = ToolExecutor(tool_registry)\n",
    "memory = WorkingMemory()\n",
    "planner = ReActPlanner(agent_llm, tool_registry)\n",
    "\n",
    "# Assemble the agent\n",
    "grounded_agent = SimpleAgent(\n",
    "    llm=agent_llm,\n",
    "    planner=planner,\n",
    "    tool_executor=executor,\n",
    "    memory=memory,\n",
    "    max_steps=5\n",
    ")\n",
    "\n",
    "grounded_agent.role_description = (\n",
    "    \"You are a UCCS student assistant. \"\n",
    "    \"Your job is to answer student questions about UCCS accurately. \"\n",
    "    \"ALWAYS search the knowledge base before answering - never make up information. \"\n",
    "    \"Use the course_knowledge_query tool to find relevant information, then base your answer on those documents. \"\n",
    "    \"If the information isn't in the knowledge base, say so and suggest where students can find more information.\"\n",
    ")\n",
    "\n",
    "print(\"✓ Grounded agent created successfully!\")\n",
    "print(\"✓ Agent has access to UCCS knowledge base via RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Grounded Agent\n",
    "\n",
    "Let's ask the same question - but now the agent has access to real documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_grounded_agent(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nAgent thinking...\\n\")\n",
    "    \n",
    "    response = await grounded_agent.arun(question)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(response)\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Question about library hours (the one that was hallucinated before!)\n",
    "await test_grounded_agent(\"What time does the Kraemer Family Library at UCCS close on Friday nights?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Question about degree requirements\n",
    "await test_grounded_agent(\"How many credit hours are required to graduate with a Computer Science degree from UCCS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Question about campus resources\n",
    "await test_grounded_agent(\"Where can I get help with my resume at UCCS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Question about something NOT in the documents\n",
    "await test_grounded_agent(\"What is the tuition cost for in-state students at UCCS?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 2\n",
    "\n",
    "**What's different between the pure LLM and the grounded agent?**\n",
    "\n",
    "Notice:\n",
    "- The grounded agent **searches documents first** before answering\n",
    "- Answers are **based on actual source text** (you can verify!)\n",
    "- When info isn't available, it **admits it and suggests alternatives** (no hallucination!)\n",
    "- You can **trace where the information came from**\n",
    "\n",
    "This is **Retrieval-Augmented Generation (RAG)** in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RAG Pipeline\n",
    "\n",
    "### Without Grounding (Pure LLM):\n",
    "```\n",
    "User: \"What's the library closing time on Friday?\"\n",
    "  ↓\n",
    "LLM: [Predicts based on general patterns]\n",
    "  ↓\n",
    "LLM: \"Most libraries close around 10 PM on Fridays...\" (HALLUCINATION!)\n",
    "```\n",
    "\n",
    "### With Grounding (RAG Agent):\n",
    "```\n",
    "User: \"What's the library closing time on Friday?\"\n",
    "  ↓\n",
    "Agent: [Uses course_knowledge_query tool]\n",
    "  ↓\n",
    "KnowledgeBaseQueryTool: [Calls retriever]\n",
    "  ↓\n",
    "SimpleRetriever: [Queries vector store]\n",
    "  ↓\n",
    "ChromaDBVectorStore: [Semantic search via embeddings]\n",
    "  ↓\n",
    "Returns: \"Friday: 7:30 AM - 6:00 PM\" from library_hours.txt\n",
    "  ↓\n",
    "Agent: [Generates answer based on retrieved text]\n",
    "  ↓\n",
    "Agent: \"According to the library hours, Kraemer Library closes at 6:00 PM on Fridays.\"\n",
    "```\n",
    "\n",
    "### The RAG Architecture Layers:\n",
    "```\n",
    "Agent (Decision Making)\n",
    "   |\n",
    "KnowledgeBaseQueryTool (Interface)\n",
    "   |\n",
    "SimpleRetriever (Query Logic)\n",
    "   |\n",
    "LongTermMemory (Abstraction)\n",
    "   |\n",
    "ChromaDBVectorStore (Storage + Search)\n",
    "   |\n",
    "SentenceTransformerEmbedder (Vectorization)\n",
    "```\n",
    "\n",
    "Each layer has a specific responsibility:\n",
    "- **Embedder**: Converts text to vectors\n",
    "- **Vector Store**: Stores and searches embeddings\n",
    "- **Long-Term Memory**: Provides memory abstraction\n",
    "- **Retriever**: Implements retrieval logic\n",
    "- **Tool**: Formats results for the agent\n",
    "- **Agent**: Makes decisions about when to use the tool\n",
    "\n",
    "**Key Benefits:**\n",
    "- ✅ Factual and verifiable\n",
    "- ✅ Based on YOUR documents\n",
    "- ✅ Up-to-date (just update the docs!)\n",
    "- ✅ Transparent (you can see the sources)\n",
    "- ✅ Modular (swap components as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Committees of AI Agents\n",
    "\n",
    "## Why Use Multiple Agents?\n",
    "\n",
    "Just like human teams, different agents can have different:\n",
    "- **Roles** (specialist vs. generalist)\n",
    "- **Tools** (some agents have access to certain resources)\n",
    "- **Expertise** (some are better at specific tasks)\n",
    "\n",
    "### Real-World Examples:\n",
    "- **Code Review Committee**: One agent writes code, another reviews for bugs, another checks style\n",
    "- **Essay Grading Team**: One checks grammar, another evaluates argument quality, another verifies facts\n",
    "- **Research Team**: One searches papers, another summarizes, another synthesizes findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Task: Essay Evaluation\n",
    "\n",
    "Let's build a committee to grade essays with three specialized agents:\n",
    "1. **Grammar Agent**: Checks spelling, grammar, and clarity\n",
    "2. **Content Agent**: Evaluates argument quality and evidence\n",
    "3. **Coordinator Agent**: Synthesizes feedback and assigns final grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Essay for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_essay = \"\"\"The Impact of Artificial Intelligence on Education\n",
    "\n",
    "Artificial intelligence is revolutionizing education in many ways. AI can personalize learning \n",
    "by adapting to each students pace and style. For example, intelligent tutoring systems can \n",
    "identify where a student struggles and provide targeted practice.\n",
    "\n",
    "However, their are concerns about AI in education. Some worry that students might become to \n",
    "dependent on AI tools and not develop critical thinking skills. Others point out that AI \n",
    "systems can perpetuate biases if there trained on biased data.\n",
    "\n",
    "Despite these challenges, the benefits outweigh the risks. AI can help teachers by automating \n",
    "administrative tasks, allowing them to focus on actual teaching. It can also make education \n",
    "more accessible to students in remote areas who otherwise wouldn't have access to quality \n",
    "instruction.\n",
    "\n",
    "In conclusion, AI has the potential to transform education for the better, but we must \n",
    "implement it thoughtfully and address the ethical concerns. The future of education will \n",
    "likely involve a partnership between human teachers and AI systems, combining the best of both.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample essay loaded!\")\n",
    "print(f\"Length: {len(sample_essay.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Specialized Agent Tools\n",
    "\n",
    "First, let's create tools that represent specialized capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarCheckTool(AbstractTool):\n",
    "    \"\"\"Tool for checking grammar and writing quality\"\"\"\n",
    "    \n",
    "    name = \"check_grammar\"\n",
    "    description = (\n",
    "        \"Analyzes text for grammar, spelling, and clarity issues. \"\n",
    "        \"Input: essay text. Returns: detailed grammar feedback.\"\n",
    "    )\n",
    "    \n",
    "    def use(self, tool_input: str) -> str:\n",
    "        \"\"\"Simulate grammar checking\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Simple checks (real systems use NLP libraries like LanguageTool)\n",
    "        if \"their are\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'their are' - should be 'there are' (wrong homophone)\")\n",
    "        if \"to dependent\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'to dependent' - should be 'too dependent' (wrong homophone)\")\n",
    "        if \"students pace\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'students pace' - missing apostrophe: 'student's pace'\")\n",
    "        if \"there trained\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'there trained' - should be 'they're trained' (contraction needed)\")\n",
    "        \n",
    "        if issues:\n",
    "            return \"Grammar Issues Found:\\n\" + \"\\n\".join(issues) + \"\\n\\nOverall: Multiple homophone and apostrophe errors detected.\"\n",
    "        else:\n",
    "            return \"No major grammar issues detected. Writing is clear and correct.\"\n",
    "\n",
    "\n",
    "class ContentAnalysisTool(AbstractTool):\n",
    "    \"\"\"Tool for analyzing argument quality and evidence\"\"\"\n",
    "    \n",
    "    name = \"analyze_content\"\n",
    "    description = (\n",
    "        \"Evaluates the quality of arguments and evidence in an essay. \"\n",
    "        \"Input: essay text. Returns: content quality assessment.\"\n",
    "    )\n",
    "    \n",
    "    def use(self, tool_input: str) -> str:\n",
    "        \"\"\"Simulate content analysis\"\"\"\n",
    "        text_lower = tool_input.lower()\n",
    "        \n",
    "        # Count evidence markers\n",
    "        evidence_markers = [\"for example\", \"research shows\", \"studies indicate\", \"data suggests\"]\n",
    "        evidence_count = sum(1 for marker in evidence_markers if marker in text_lower)\n",
    "        \n",
    "        # Check structure\n",
    "        has_intro = \"introduction\" in text_lower or tool_input.strip().startswith((\"Artificial\", \"The\", \"In\"))\n",
    "        has_conclusion = \"conclusion\" in text_lower or \"in summary\" in text_lower\n",
    "        \n",
    "        # Check for counterarguments\n",
    "        has_counterargument = any(word in text_lower for word in [\"however\", \"although\", \"despite\", \"concern\"])\n",
    "        \n",
    "        feedback = \"Content Analysis Results:\\n\\n\"\n",
    "        feedback += f\"Structure:\\n\"\n",
    "        feedback += f\"  - Clear introduction: {'Yes' if has_intro else 'No'}\\n\"\n",
    "        feedback += f\"  - Clear conclusion: {'Yes' if has_conclusion else 'No'}\\n\\n\"\n",
    "        feedback += f\"Argumentation:\\n\"\n",
    "        feedback += f\"  - Evidence examples provided: {evidence_count}\\n\"\n",
    "        feedback += f\"  - Addresses counterarguments: {'Yes' if has_counterargument else 'No'}\\n\\n\"\n",
    "        \n",
    "        if evidence_count < 2:\n",
    "            feedback += \"Suggestion: Add more specific examples and evidence to support claims. \"\n",
    "            feedback += \"Consider citing research studies or real-world data.\\n\"\n",
    "        if has_counterargument:\n",
    "            feedback += \"Strength: Essay acknowledges opposing views, showing critical thinking.\\n\"\n",
    "        \n",
    "        return feedback\n",
    "\n",
    "\n",
    "# Create the tools\n",
    "grammar_tool = GrammarCheckTool()\n",
    "content_tool = ContentAnalysisTool()\n",
    "\n",
    "print(\"✓ Specialized tools created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Specialized Agents\n",
    "\n",
    "Now we'll create three agents with different roles and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_worker_agent(llm, tools, role_description):\n",
    "    \"\"\"\n",
    "    Factory function to create specialized worker agents.\n",
    "    Each worker gets its own tool registry, planner, executor, and memory.\n",
    "    \"\"\"\n",
    "    tool_registry = ToolRegistry()\n",
    "    for tool in tools:\n",
    "        tool_registry.register_tool(tool)\n",
    "    \n",
    "    planner = ReActPlanner(llm, tool_registry)\n",
    "    executor = ToolExecutor(tool_registry)\n",
    "    memory = WorkingMemory()\n",
    "    \n",
    "    # Create a stateless agent (important for workers!)\n",
    "    agent = SimpleAgent(llm, planner, executor, memory, stateless=True)\n",
    "    \n",
    "    # Set role description\n",
    "    agent.role_description = role_description\n",
    "    \n",
    "    return agent\n",
    "\n",
    "print(\"✓ Worker agent factory function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker 1: Grammar Specialist\n",
    "print(\"Creating Grammar Specialist worker...\")\n",
    "\n",
    "grammar_llm = HuggingFaceAdapter(\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "\n",
    "grammar_agent = create_worker_agent(\n",
    "    llm=grammar_llm,\n",
    "    tools=[grammar_tool],\n",
    "    role_description=(\n",
    "        \"A Grammar Specialist and writing instructor. \"\n",
    "        \"Your job is to evaluate grammar, spelling, and writing clarity. \"\n",
    "        \"Use the check_grammar tool to analyze text, then provide specific feedback. \"\n",
    "        \"Rate grammar quality on a scale of 1-10 and explain your rating.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ Grammar Specialist worker created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker 2: Content Specialist\n",
    "print(\"Creating Content Specialist worker...\")\n",
    "\n",
    "content_llm = HuggingFaceAdapter(\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "\n",
    "content_agent = create_worker_agent(\n",
    "    llm=content_llm,\n",
    "    tools=[content_tool],\n",
    "    role_description=(\n",
    "        \"A Content Specialist and critical thinking instructor. \"\n",
    "        \"Your job is to evaluate argument quality, evidence, and logical structure. \"\n",
    "        \"Use the analyze_content tool to assess text. \"\n",
    "        \"Rate content quality on a scale of 1-10 and explain your rating.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ Content Specialist worker created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize workers in a dictionary for the manager\n",
    "workers = {\n",
    "    \"GrammarSpecialist\": grammar_agent,\n",
    "    \"ContentSpecialist\": content_agent\n",
    "}\n",
    "\n",
    "print(f\"✓ Team assembled with {len(workers)} specialist workers\")\n",
    "print(f\"  Workers: {', '.join(workers.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Manager Agent (coordinator)\n",
    "print(\"\\nCreating Manager Agent (Lead Evaluator)...\")\n",
    "\n",
    "manager_llm = HuggingFaceAdapter(\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "\n",
    "# Create memory for the manager\n",
    "manager_memory = WorkingMemory()\n",
    "\n",
    "# Create the specialized ManagerPlanner\n",
    "manager_planner = ManagerPlanner(manager_llm, workers)\n",
    "\n",
    "# Create manager agent (note: executor is None - manager only delegates!)\n",
    "manager_agent = SimpleAgent(\n",
    "    llm=manager_llm,\n",
    "    planner=manager_planner,\n",
    "    tool_executor=None,  # Manager doesn't execute tools directly\n",
    "    memory=manager_memory\n",
    ")\n",
    "\n",
    "# Set manager's role\n",
    "manager_agent.role_description = (\n",
    "    \"You are the Lead Evaluator managing an essay grading team. \"\n",
    "    \"Delegate tasks to GrammarSpecialist and ContentSpecialist workers. \"\n",
    "    \"Once you have both reports, synthesize them into a final grade and feedback.\"\n",
    ")\n",
    "\n",
    "print(\"✓ Manager Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Committee\n",
    "\n",
    "Now let's coordinate the agents to evaluate our essay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hierarchical Agent Runner\n",
    "print(\"\\nInitializing Hierarchical Agent Runner...\")\n",
    "\n",
    "essay_committee = HierarchicalAgentRunner(\n",
    "    manager_agent=manager_agent,\n",
    "    workers=workers,\n",
    "    max_steps=10  # Maximum coordination steps\n",
    ")\n",
    "\n",
    "print(\"✓ Essay Evaluation Committee assembled!\")\n",
    "print(\"\\nCommittee Structure:\")\n",
    "print(\"  Manager: Lead Evaluator\")\n",
    "print(\"  ├─ GrammarSpecialist\")\n",
    "print(\"  └─ ContentSpecialist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_essay_with_committee(essay_text):\n",
    "    \"\"\"\n",
    "    Coordinate multiple agents to evaluate an essay using hierarchical delegation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\" \" * 20 + \"ESSAY EVALUATION COMMITTEE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create the evaluation request for the manager\n",
    "    evaluation_request = f\"\"\"\n",
    "Please evaluate this essay comprehensively:\n",
    "\n",
    "ESSAY:\n",
    "{essay_text}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. First, delegate to GrammarSpecialist to evaluate grammar and writing quality\n",
    "2. Then, delegate to ContentSpecialist to evaluate content and argumentation\n",
    "3. Finally, synthesize both reports into:\n",
    "   - Overall Grade (A, B, C, D, or F)\n",
    "   - Summary of Strengths (2-3 points)\n",
    "   - Key Areas for Improvement (top 3)\n",
    "   - One Actionable Next Step for the student\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"\\n[MANAGER COORDINATING EVALUATION...]\\n\")\n",
    "    \n",
    "    # Run the hierarchical agent team\n",
    "    final_evaluation = await essay_committee.arun(evaluation_request)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 25 + \"FINAL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(final_evaluation)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the committee evaluation!\n",
    "await evaluate_essay_with_committee(sample_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 3\n",
    "\n",
    "**What advantages does the hierarchical multi-agent approach have?**\n",
    "\n",
    "Think about:\n",
    "- **Manager-Worker Pattern**: Clear delegation and coordination\n",
    "- **Specialization**: Each worker focuses on one task with dedicated tools\n",
    "- **Stateless Workers**: Workers don't maintain conversation history (more efficient)\n",
    "- **Automatic Orchestration**: `HierarchicalAgentRunner` manages the workflow\n",
    "- **Synthesis by Manager**: Manager combines results into coherent final answer\n",
    "- **Scalability**: Easy to add more specialists to the team\n",
    "\n",
    "**How is this different from our earlier sequential approach?**\n",
    "- Sequential: We manually called each agent and passed results around\n",
    "- Hierarchical: Manager decides when and how to delegate automatically\n",
    "- Hierarchical: Manager synthesizes results intelligently based on observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Hierarchical Multi-Agent Systems\n",
    "\n",
    "### The Architecture:\n",
    "```\n",
    "User Query\n",
    "    ↓\n",
    "Manager Agent (ManagerPlanner)\n",
    "    ├─→ Delegates to GrammarSpecialist\n",
    "    │   └─→ Returns grammar report\n",
    "    ├─→ Delegates to ContentSpecialist\n",
    "    │   └─→ Returns content report\n",
    "    └─→ Synthesizes final evaluation\n",
    "    ↓\n",
    "Final Answer to User\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "**ManagerPlanner:**\n",
    "- Special planner that understands delegation\n",
    "- Has two \"tools\": `delegate` and `final_answer`\n",
    "- Decides which worker to use based on the task\n",
    "\n",
    "**Worker Agents:**\n",
    "- Stateless (don't maintain conversation history)\n",
    "- Each has specific tools and expertise\n",
    "- Return results to manager via observations\n",
    "\n",
    "**HierarchicalAgentRunner:**\n",
    "- Orchestrates the entire workflow\n",
    "- Routes delegations to correct workers\n",
    "- Passes worker results back to manager as observations\n",
    "- Manages the coordination loop\n",
    "\n",
    "### Why This Pattern Works:\n",
    "\n",
    "1. **Clear Separation**: Manager thinks strategically, workers execute tactically\n",
    "2. **Flexible Coordination**: Manager can adapt based on worker results\n",
    "3. **Production-Ready**: Same pattern used in complex enterprise systems\n",
    "4. **Explainable**: You can see the entire decision chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Test the committee on your own essay or writing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_essay = \"\"\"\n",
    "Paste your essay here!\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to test:\n",
    "# await evaluate_essay_with_committee(your_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights: Multi-Agent Systems\n",
    "\n",
    "### Why Committees Beat Single Agents:\n",
    "\n",
    "**Specialization**:\n",
    "- Each agent masters one skill\n",
    "- Like human experts, focused agents perform better\n",
    "- Reduces cognitive load per agent\n",
    "\n",
    "**Scalability**:\n",
    "- Add new specialists as needed\n",
    "- Agents can work in parallel for speed\n",
    "- Modular architecture is easier to maintain\n",
    "\n",
    "**Reliability**:\n",
    "- Multiple perspectives catch more issues\n",
    "- One agent's weakness is another's strength\n",
    "- Cross-validation reduces errors\n",
    "\n",
    "**Transparency**:\n",
    "- See each agent's reasoning\n",
    "- Easier to debug and improve\n",
    "- Clear audit trail\n",
    "\n",
    "### When to Use Committees:\n",
    "- ✅ Complex tasks requiring multiple skills\n",
    "- ✅ Quality-critical applications (grading, review, analysis)\n",
    "- ✅ Tasks that benefit from different perspectives\n",
    "- ✅ When you need explainability and transparency\n",
    "\n",
    "### When NOT to Use Committees:\n",
    "- ❌ Simple queries (overkill and slower)\n",
    "- ❌ Extremely time-sensitive tasks (coordination adds latency)\n",
    "- ❌ Tasks requiring deep context sharing (single agent is easier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Combining Grounding + Committees\n",
    "\n",
    "The most powerful systems combine BOTH techniques:\n",
    "\n",
    "```\n",
    "Example: Fact-Checked Essay Grader\n",
    "\n",
    "Grammar Agent (specialist)\n",
    "     ↓\n",
    "Content Agent (specialist)\n",
    "     ↓\n",
    "Fact-Checker Agent (grounded on knowledge base via RAG)\n",
    "     ↓\n",
    "Coordinator (synthesizes all feedback)\n",
    "     ↓\n",
    "Final Report with verified facts\n",
    "```\n",
    "\n",
    "Each agent is:\n",
    "- **Grounded** (has access to relevant knowledge via RAG)\n",
    "- **Specialized** (focuses on one task)\n",
    "- **Coordinated** (works with other agents)\n",
    "\n",
    "### Real-World Application Ideas:\n",
    "1. **Research Assistant**: Search agent (RAG on papers) → Summarizer → Synthesizer\n",
    "2. **Code Review**: Linter → Security Checker (grounded on CVE database) → Style Reviewer\n",
    "3. **Customer Support**: Intent Classifier → Knowledge Base Agent (RAG) → Response Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "### 1. How does grounding solve the hallucination problem?\n",
    "### 2. When would you use RAG vs. fine-tuning a model?\n",
    "### 3. What are the tradeoffs between single agents and committees?\n",
    "### 4. Can you think of a real-world task that would benefit from BOTH grounding AND multiple agents?\n",
    "### 5. How might you apply these techniques to your final project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've learned advanced agentic AI techniques:\n",
    "\n",
    "### Grounding Foundation Models:\n",
    "- ✓ Why LLMs hallucinate\n",
    "- ✓ How RAG works (Retrieve → Augment → Generate)\n",
    "- ✓ Using DocumentProcessor to load documents\n",
    "- ✓ Creating SimpleRetriever for semantic search\n",
    "- ✓ Building agents with KnowledgeBaseQueryTool\n",
    "- ✓ Verifiable, source-backed responses\n",
    "\n",
    "### Multi-Agent Committees:\n",
    "- ✓ Agent specialization and roles\n",
    "- ✓ Coordinating multiple agents\n",
    "- ✓ Sequential and parallel workflows\n",
    "- ✓ Synthesizing diverse feedback\n",
    "- ✓ When to use committees vs. single agents\n",
    "\n",
    "### What's Next?\n",
    "- Experiment with different committee structures\n",
    "- Build agents grounded on YOUR data (PDFs, docs, websites)\n",
    "- Combine techniques for your final projects\n",
    "- Explore advanced patterns (debate, voting, hierarchies)\n",
    "- Build production-ready multi-agent systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Challenges (Optional)\n",
    "\n",
    "If you want to explore further, try these:\n",
    "\n",
    "### Challenge 1: Expand the Knowledge Base\n",
    "Add more UCCS documents (parking info, dining services, athletics) and test the grounded agent.\n",
    "\n",
    "### Challenge 2: Add a Third Specialist\n",
    "Create a \"Citation Checker\" or \"Fact Verifier\" agent that uses RAG to verify claims in essays.\n",
    "\n",
    "### Challenge 3: Parallel Evaluation\n",
    "Modify the committee to run grammar and content agents in parallel using `asyncio.gather()`:\n",
    "```python\n",
    "grammar_task, content_task = await asyncio.gather(\n",
    "    grammar_agent.arun(prompt),\n",
    "    content_agent.arun(prompt)\n",
    ")\n",
    "```\n",
    "\n",
    "### Challenge 4: Grounded Fact-Checker\n",
    "Add a fourth agent that uses the knowledge base to verify factual claims in essays.\n",
    "\n",
    "### Challenge 5: Your Own Multi-Agent System\n",
    "Design a committee for a task relevant to your final project. Consider:\n",
    "- What specialists are needed?\n",
    "- Which agents need RAG access?\n",
    "- Sequential or parallel coordination?\n",
    "- How to synthesize results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Demos)",
   "language": "python",
   "name": "llm-math-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
