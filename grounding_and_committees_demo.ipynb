{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Advanced Agentic AI\n",
    "\n",
    "## Grounding Foundation Models & Multi-Agent Committees\n",
    "\n",
    "In Lesson 1, you learned that LLMs are powerful pattern predictors but have limitations:\n",
    "- They can't access real-time information\n",
    "- They don't know about YOUR specific data\n",
    "- They work in isolation\n",
    "\n",
    "Today, we'll solve these problems by:\n",
    "1. **Grounding** LLMs with your own knowledge base\n",
    "2. **Coordinating** multiple agents to solve complex tasks\n",
    "\n",
    "# **Guiding Questions:**\n",
    "1. How can we make LLMs answer questions about information they've never seen?\n",
    "2. Can multiple AI agents work together better than one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Grounding Foundation Models\n",
    "\n",
    "## What Does \"Grounding\" Mean?\n",
    "\n",
    "**Grounding** means connecting an LLM's responses to specific, verifiable sources of information.\n",
    "\n",
    "### The Problem:\n",
    "- LLMs only know what was in their training data (cutoff: early 2024 for most models)\n",
    "- They hallucinate when they don't know something\n",
    "- They can't access YOUR documents, databases, or private information\n",
    "\n",
    "### The Solution: RAG (Retrieval-Augmented Generation)\n",
    "**RAG** = Retrieve relevant information â†’ Augment the prompt â†’ Generate grounded responses\n",
    "\n",
    "Think of it like an open-book exam vs. a closed-book exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "Let's set up our environment for building a grounded agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from fairlib.utils.document_processor import DocumentProcessor\n",
    "\n",
    "from fairlib import (\n",
    "    settings,\n",
    "    Message,\n",
    "    HuggingFaceAdapter,\n",
    "    ToolRegistry,\n",
    "    ToolExecutor,\n",
    "    WorkingMemory,\n",
    "    LongTermMemory,\n",
    "    ChromaDBVectorStore,\n",
    "    ReActPlanner,\n",
    "    SimpleAgent,\n",
    "    SentenceTransformerEmbedder,\n",
    "    SimpleRetriever,\n",
    "    KnowledgeBaseQueryTool  # <-- Using the official framework tool\n",
    ")\n",
    "\n",
    "# ChromaDB for vector storage\n",
    "try:\n",
    "    import chromadb\n",
    "    CHROMADB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: chromadb not installed. Install with: pip install chromadb\")\n",
    "    CHROMADB_AVAILABLE = False\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "if not token:\n",
    "    print(\"Warning: HUGGING_FACE_HUB_TOKEN not found in .env file!\")\n",
    "else:\n",
    "    print(\"Token loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: The Hallucination Problem\n",
    "\n",
    "Let's see what happens when we ask an LLM about information it doesn't have.\n",
    "\n",
    "**Try this**: Google \"UCCS Kraemer Library hours\" and see what the actual hours are. Then compare to what the LLM says!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model...\n",
      "ðŸ”§ Loading HuggingFace model: cognitivecomputations/Dolphin3.0-Qwen2.5-3b (quantized=False, stream=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load a language model\n",
    "print(\"Loading language model...\")\n",
    "llm = HuggingFaceAdapter(\n",
    "    model_name=\"dolphin3-qwen25-3b\", \n",
    "    auth_token=token,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What time does the Kraemer Family Library at UCCS close on Friday nights?\n",
      "\n",
      "LLM Response:\n",
      "The Kraemer Family Library at UCCS typically closes around 5:00 PM on Fridays, but it's always best to check their official website or contact them directly for the most accurate information as hours can vary depending on the day of the week and other factors.\n",
      "\n",
      "WARNING: This information might be completely made up!\n",
      "\n",
      "Google 'What time does the Kraemer Family Library at UCCS close on Friday nights?' to verify if this is correct!\n"
     ]
    }
   ],
   "source": [
    "# Ask about UCCS library hours\n",
    "question = \"What time does the Kraemer Family Library at UCCS close on Friday nights?\"\n",
    "\n",
    "messages = [Message(role=\"user\", content=question)]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nLLM Response:\\n{response.content}\")\n",
    "print(\"\\nWARNING: This information might be completely made up!\")\n",
    "print(\"\\nGoogle 'What time does the Kraemer Family Library at UCCS close on Friday nights?' to verify if this is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 1\n",
    "\n",
    "**Did the LLM give you the right answer? Did it admit it doesn't know, or did it make something up?**\n",
    "\n",
    "This is the hallucination problem: LLMs try to answer even when they don't have the information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Knowledge Base with Real Documents\n",
    "\n",
    "Instead of relying on the LLM's training data, let's create actual documents with UCCS information.\n",
    "\n",
    "In a real system, you'd have PDFs, Word docs, or web pages. For this demo, we'll create text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base documents created!\n",
      "Saved to: ./uccs_knowledge_base\n",
      "Documents: library_hours.txt, degree_requirements.txt, campus_resources.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for our knowledge base documents\n",
    "kb_directory = \"./uccs_knowledge_base\"\n",
    "os.makedirs(kb_directory, exist_ok=True)\n",
    "\n",
    "# Document 1: Library Hours\n",
    "library_hours_doc = \"\"\"Kraemer Family Library Hours (UCCS)\n",
    "\n",
    "Fall/Spring Semester Regular Hours:\n",
    "- Monday - Thursday: 7:30 AM - 11:00 PM\n",
    "- Friday: 7:30 AM - 6:00 PM\n",
    "- Saturday: 10:00 AM - 6:00 PM\n",
    "- Sunday: 12:00 PM - 11:00 PM\n",
    "\n",
    "Finals Week Extended Hours:\n",
    "- Sunday - Thursday: 7:30 AM - 2:00 AM\n",
    "- Friday: 7:30 AM - 6:00 PM\n",
    "- Saturday: 10:00 AM - 6:00 PM\n",
    "\n",
    "Summer and Break Hours:\n",
    "- Monday - Friday: 8:00 AM - 5:00 PM\n",
    "- Closed on weekends\n",
    "\n",
    "Note: Hours may vary during holidays. Always check library.uccs.edu for the most current information.\n",
    "\"\"\"\n",
    "\n",
    "# Document 2: Degree Requirements\n",
    "degree_requirements_doc = \"\"\"UCCS Computer Science B.S. Degree Requirements\n",
    "\n",
    "Total Credit Hours Required: 120\n",
    "\n",
    "Core Computer Science Courses: 48 credit hours\n",
    "- CS 1030, 1050, 1150, 1200, 2060, 2100, 2400\n",
    "- CS 3100, 3150, 3300, 3400, 3500, 4200\n",
    "- CS 4310 (Senior Design I) and CS 4320 (Senior Design II)\n",
    "\n",
    "Mathematics Requirements: 16 credit hours\n",
    "- MATH 1310, 1320, 2130, 2420\n",
    "\n",
    "General Education: 35 credit hours\n",
    "- UCCS Core and Compass Curriculum requirements\n",
    "\n",
    "Electives and Free Choice: 21 credit hours\n",
    "\n",
    "Additional Requirements:\n",
    "- Minimum 2.0 GPA in major courses\n",
    "- At least 30 credit hours must be upper-division (3000-4000 level)\n",
    "- Senior design capstone project required\n",
    "- Complete at least 45 credit hours at UCCS\n",
    "\"\"\"\n",
    "\n",
    "# Document 3: Campus Resources\n",
    "campus_resources_doc = \"\"\"UCCS Student Resources and Services\n",
    "\n",
    "Writing Center:\n",
    "- Location: Columbine Hall, Room 108\n",
    "- Services: Free writing tutoring for all UCCS students\n",
    "- Walk-in hours: Monday-Friday, 9:00 AM - 5:00 PM\n",
    "- Online appointments available via Zoom\n",
    "- Website: writingcenter.uccs.edu\n",
    "\n",
    "Wellness Center:\n",
    "- Location: University Center, Second Floor\n",
    "- Services: Medical care, counseling, health education\n",
    "- Phone: 719-255-4444\n",
    "- Hours: Monday-Friday, 8:00 AM - 5:00 PM\n",
    "- Crisis support available 24/7\n",
    "\n",
    "Career Center:\n",
    "- Location: Cragmor Hall, Room 111\n",
    "- Services: Resume reviews, interview preparation, job search assistance\n",
    "- Handshake platform for job/internship postings\n",
    "- Career fairs held each semester (Fall and Spring)\n",
    "- One-on-one advising appointments available\n",
    "\n",
    "Math Learning Center:\n",
    "- Location: Engineering Building, Room 105\n",
    "- Free tutoring for math courses up to Calculus II\n",
    "- Walk-in hours: Monday-Thursday, 10:00 AM - 6:00 PM\n",
    "\n",
    "Computer Science Tutoring Lab:\n",
    "- Location: Engineering Building, Room 207\n",
    "- Free tutoring for CS 1030, 1050, 1150, 1200, 2060\n",
    "- Hours: Monday-Friday, 12:00 PM - 5:00 PM\n",
    "\"\"\"\n",
    "\n",
    "# Save documents to files\n",
    "with open(os.path.join(kb_directory, \"library_hours.txt\"), \"w\") as f:\n",
    "    f.write(library_hours_doc)\n",
    "\n",
    "with open(os.path.join(kb_directory, \"degree_requirements.txt\"), \"w\") as f:\n",
    "    f.write(degree_requirements_doc)\n",
    "\n",
    "with open(os.path.join(kb_directory, \"campus_resources.txt\"), \"w\") as f:\n",
    "    f.write(campus_resources_doc)\n",
    "\n",
    "print(\"Knowledge base documents created!\")\n",
    "print(f\"Saved to: {kb_directory}\")\n",
    "print(f\"Documents: library_hours.txt, degree_requirements.txt, campus_resources.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents with FAIR_LLM\n",
    "\n",
    "Now we'll use FAIR_LLM's `DocumentProcessor` to load and chunk these documents.\n",
    "\n",
    "This is the same process you'd use for PDFs, Word docs, PowerPoints, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processor initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the document processor\n",
    "doc_processor = DocumentProcessor(config={\n",
    "    \"files_directory\": kb_directory,\n",
    "    \"max_chunk_chars\": 1000,\n",
    "    \"supported_extensions\": {\".txt\", \".pdf\", \".docx\"}\n",
    "})\n",
    "\n",
    "print(\"Document processor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and chunking documents...\n",
      "\n",
      "\n",
      "Loaded 8 document chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Kraemer Family Library Hours (UCCS) Fall/Spring Semester Regular Hours:\n",
      "- Monday - Thursday: 7:30 AM - 11:00 PM\n",
      "- Friday: 7:30 AM - 6:00 PM\n",
      "- Saturday: 10:00 AM - 6:00 PM\n",
      "- Sunday: 12:00 PM - 11:00 PM...\n",
      "\n",
      "Metadata: {'filename': 'library_hours.txt', 'type': '.txt', 'segment_index': 0, 'chunk_index': 0, 'source': 'library_hours.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Load all documents from the folder\n",
    "print(\"Loading and chunking documents...\\n\")\n",
    "documents = doc_processor.load_documents_from_folder()\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} document chunks\")\n",
    "print(f\"\\nExample chunk:\")\n",
    "print(f\"Content: {documents[0].page_content[:200]}...\")\n",
    "print(f\"\\nMetadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG Architecture\n",
    "\n",
    "Now we'll build the proper RAG pipeline with the core FAIR_LLM components:\n",
    "\n",
    "1. **Embedder**: Converts text into numerical vectors\n",
    "2. **Vector Store**: Stores embeddings and enables similarity search  \n",
    "3. **Long-Term Memory**: Wraps the vector store\n",
    "4. **Retriever**: Queries the vector store to find relevant documents\n",
    "\n",
    "**How it works:**\n",
    "1. Documents are converted into vector embeddings (numerical representations)\n",
    "2. Embeddings are stored in a vector database (ChromaDB)\n",
    "3. When you query, the query is converted to a vector\n",
    "4. Vector store finds the most similar document vectors (semantic search)\n",
    "5. Retriever returns the most relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB available\n"
     ]
    }
   ],
   "source": [
    "# Check if ChromaDB is available\n",
    "if not CHROMADB_AVAILABLE:\n",
    "    print(\"ERROR: ChromaDB is required for this section.\")\n",
    "    print(\"Install it with: pip install chromadb\")\n",
    "    print(\"\\nSkipping RAG setup...\")\n",
    "else:\n",
    "    print(\"ChromaDB available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating embedder...\n",
      "âœ… SentenceTransformerEmbedder initialized with model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ“ Embedder created\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the embedder\n",
    "print(\"Step 1: Creating embedder...\")\n",
    "embedder = SentenceTransformerEmbedder(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"  # Fast, efficient model\n",
    ")\n",
    "print(\"âœ“ Embedder created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2 & 3: Creating vector store and adding documents...\n",
      "âœ“ Vector store created\n",
      "  Total chunks: 8, Unique chunks: 5\n",
      "âœ“ Added 5 document chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "# Step 2 & 3: Create vector store and add documents (combined to prevent errors)\n",
    "print(\"\\nStep 2 & 3: Creating vector store and adding documents...\")\n",
    "\n",
    "# Create a fresh ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Delete collection if it exists (prevents duplicate IDs on reruns)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"uccs_knowledge_base\")\n",
    "    print(\"  (Cleared existing collection)\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create fresh vector store\n",
    "vector_store = ChromaDBVectorStore(\n",
    "    embedder=embedder,\n",
    "    client=chroma_client,\n",
    "    collection_name=\"uccs_knowledge_base\"\n",
    ")\n",
    "print(\"âœ“ Vector store created\")\n",
    "\n",
    "# Extract text content from Document objects\n",
    "document_texts = [doc.page_content for doc in documents]\n",
    "\n",
    "# CRITICAL: Remove duplicate documents (ChromaDB uses hash(doc) as ID internally)\n",
    "unique_texts = []\n",
    "seen = set()\n",
    "for text in document_texts:\n",
    "    if text not in seen:\n",
    "        seen.add(text)\n",
    "        unique_texts.append(text)\n",
    "\n",
    "print(f\"  Total chunks: {len(document_texts)}, Unique chunks: {len(unique_texts)}\")\n",
    "\n",
    "# Add to vector store (no ids parameter - ChromaDB generates them internally)\n",
    "vector_store.add_documents(unique_texts)\n",
    "\n",
    "print(f\"âœ“ Added {len(unique_texts)} document chunks to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Creating long-term memory...\n",
      "âœ“ Long-term memory created\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create Long-Term Memory (wraps the vector store)\n",
    "print(\"\\nStep 4: Creating long-term memory...\")\n",
    "long_term_memory = LongTermMemory(vector_store)\n",
    "print(\"âœ“ Long-term memory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Creating retriever...\n",
      "âœ… SimpleRetriever initialized.\n",
      "âœ“ Retriever created and ready!\n",
      "\n",
      "============================================================\n",
      "RAG PIPELINE COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create the retriever\n",
    "print(\"\\nStep 5: Creating retriever...\")\n",
    "retriever = SimpleRetriever(vector_store)\n",
    "print(\"âœ“ Retriever created and ready!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Retriever Directly\n",
    "\n",
    "Let's see how semantic search works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'library hours friday'\n",
      "\n",
      "Top 2 most relevant chunks:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: Kraemer Family Library Hours (UCCS) Fall/Spring Semester Regular Hours:\n",
      "- Monday - Thursday: 7:30 AM - 11:00 PM\n",
      "- Friday: 7:30 AM - 6:00 PM\n",
      "- Saturday: 10:00 AM - 6:00 PM\n",
      "- Sunday: 12:00 PM - 11:00 PM Finals Week Extended Hours:\n",
      "- Sunday - Thursday: 7:30 AM - 2:00 AM\n",
      "- Friday: 7:30 AM - 6:00 PM\n",
      "- Sa...\n",
      "\n",
      "--- Result 2 ---\n",
      "Content: Kraemer Family Library Hours (UCCS) Fall/Spring Semester Regular Hours:\n",
      "- Monday - Thursday: 7:30 AM - 11:00 PM\n",
      "- Friday: 7:30 AM - 6:00 PM\n",
      "- Saturday: 10:00 AM - 6:00 PM\n",
      "- Sunday: 12:00 PM - 11:00 PM Finals Week Extended Hours:\n",
      "- Sunday - Thursday: 7:30 AM - 2:00 AM\n",
      "- Friday: 7:30 AM - 6:00 PM\n",
      "- Sa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "test_query = \"library hours friday\"\n",
    "results = retriever.retrieve(query=test_query, top_k=2)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nTop {len(results)} most relevant chunks:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    # Handle both string and Document results\n",
    "    if isinstance(result, str):\n",
    "        print(f\"Content: {result[:300]}...\\n\")\n",
    "    else:  # Document object\n",
    "        if hasattr(result, 'metadata'):\n",
    "            print(f\"Source: {result.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"Content: {result.page_content[:300] if hasattr(result, 'page_content') else str(result)[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Grounded Agent with RAG\n",
    "\n",
    "Now we'll create an agent that uses the `KnowledgeBaseQueryTool` to ground its responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering tool: course_knowledge_query\n",
      "âœ“ Grounded agent created successfully!\n",
      "âœ“ Agent has access to UCCS knowledge base via RAG\n"
     ]
    }
   ],
   "source": [
    "# Create a separate LLM instance for the agent\n",
    "agent_llm = llm\n",
    "\n",
    "# Create the knowledge base query tool\n",
    "knowledge_tool = KnowledgeBaseQueryTool(retriever=retriever)\n",
    "\n",
    "# Register the tool\n",
    "tool_registry = ToolRegistry()\n",
    "tool_registry.register_tool(knowledge_tool)\n",
    "\n",
    "# Create executor, memory, and planner\n",
    "executor = ToolExecutor(tool_registry)\n",
    "memory = WorkingMemory()\n",
    "planner = ReActPlanner(agent_llm, tool_registry)\n",
    "\n",
    "# Assemble the agent\n",
    "grounded_agent = SimpleAgent(\n",
    "    llm=agent_llm,\n",
    "    planner=planner,\n",
    "    tool_executor=executor,\n",
    "    memory=memory,\n",
    "    max_steps=5\n",
    ")\n",
    "\n",
    "grounded_agent.role_description = (\n",
    "    \"You are a UCCS student assistant. \"\n",
    "    \"Your job is to answer student questions about UCCS accurately. \"\n",
    "    \"ALWAYS search the knowledge base before answering - never make up information. \"\n",
    "    \"Use the course_knowledge_query tool to find relevant information, then base your answer on those documents. \"\n",
    "    \"If the information isn't in the knowledge base, say so and suggest where students can find more information.\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Grounded agent created successfully!\")\n",
    "print(\"âœ“ Agent has access to UCCS knowledge base via RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Grounded Agent\n",
    "\n",
    "Let's ask the same question - but now the agent has access to real documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_grounded_agent(question):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nAgent thinking...\\n\")\n",
    "    \n",
    "    response = await grounded_agent.arun(question)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Agent Response:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(response)\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Question about library hours (the one that was hallucinated before!)\n",
    "await test_grounded_agent(\"What time does the Kraemer Family Library at UCCS close on Friday nights?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Question about degree requirements\n",
    "await test_grounded_agent(\"How many credit hours are required to graduate with a Computer Science degree from UCCS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Question about campus resources\n",
    "await test_grounded_agent(\"Where can I get help with my resume at UCCS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Question about something NOT in the documents\n",
    "await test_grounded_agent(\"What is the tuition cost for in-state students at UCCS?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 2\n",
    "\n",
    "**What's different between the pure LLM and the grounded agent?**\n",
    "\n",
    "Notice:\n",
    "- The grounded agent **searches documents first** before answering\n",
    "- Answers are **based on actual source text** (you can verify!)\n",
    "- When info isn't available, it **admits it and suggests alternatives** (no hallucination!)\n",
    "- You can **trace where the information came from**\n",
    "\n",
    "This is **Retrieval-Augmented Generation (RAG)** in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RAG Pipeline\n",
    "\n",
    "### Without Grounding (Pure LLM):\n",
    "```\n",
    "User: \"What's the library closing time on Friday?\"\n",
    "  â†“\n",
    "LLM: [Predicts based on general patterns]\n",
    "  â†“\n",
    "LLM: \"Most libraries close around 10 PM on Fridays...\" (HALLUCINATION!)\n",
    "```\n",
    "\n",
    "### With Grounding (RAG Agent):\n",
    "```\n",
    "User: \"What's the library closing time on Friday?\"\n",
    "  â†“\n",
    "Agent: [Uses course_knowledge_query tool]\n",
    "  â†“\n",
    "KnowledgeBaseQueryTool: [Calls retriever]\n",
    "  â†“\n",
    "SimpleRetriever: [Queries vector store]\n",
    "  â†“\n",
    "ChromaDBVectorStore: [Semantic search via embeddings]\n",
    "  â†“\n",
    "Returns: \"Friday: 7:30 AM - 6:00 PM\" from library_hours.txt\n",
    "  â†“\n",
    "Agent: [Generates answer based on retrieved text]\n",
    "  â†“\n",
    "Agent: \"According to the library hours, Kraemer Library closes at 6:00 PM on Fridays.\"\n",
    "```\n",
    "\n",
    "### The RAG Architecture Layers:\n",
    "```\n",
    "Agent (Decision Making)\n",
    "   |\n",
    "KnowledgeBaseQueryTool (Interface)\n",
    "   |\n",
    "SimpleRetriever (Query Logic)\n",
    "   |\n",
    "LongTermMemory (Abstraction)\n",
    "   |\n",
    "ChromaDBVectorStore (Storage + Search)\n",
    "   |\n",
    "SentenceTransformerEmbedder (Vectorization)\n",
    "```\n",
    "\n",
    "Each layer has a specific responsibility:\n",
    "- **Embedder**: Converts text to vectors\n",
    "- **Vector Store**: Stores and searches embeddings\n",
    "- **Long-Term Memory**: Provides memory abstraction\n",
    "- **Retriever**: Implements retrieval logic\n",
    "- **Tool**: Formats results for the agent\n",
    "- **Agent**: Makes decisions about when to use the tool\n",
    "\n",
    "**Key Benefits:**\n",
    "- âœ… Factual and verifiable\n",
    "- âœ… Based on YOUR documents\n",
    "- âœ… Up-to-date (just update the docs!)\n",
    "- âœ… Transparent (you can see the sources)\n",
    "- âœ… Modular (swap components as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Committees of AI Agents\n",
    "\n",
    "## Why Use Multiple Agents?\n",
    "\n",
    "Just like human teams, different agents can have different:\n",
    "- **Roles** (specialist vs. generalist)\n",
    "- **Tools** (some agents have access to certain resources)\n",
    "- **Expertise** (some are better at specific tasks)\n",
    "\n",
    "### Real-World Examples:\n",
    "- **Code Review Committee**: One agent writes code, another reviews for bugs, another checks style\n",
    "- **Essay Grading Team**: One checks grammar, another evaluates argument quality, another verifies facts\n",
    "- **Research Team**: One searches papers, another summarizes, another synthesizes findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Task: Essay Evaluation\n",
    "\n",
    "Let's build a committee to grade essays with three specialized agents:\n",
    "1. **Grammar Agent**: Checks spelling, grammar, and clarity\n",
    "2. **Content Agent**: Evaluates argument quality and evidence\n",
    "3. **Coordinator Agent**: Synthesizes feedback and assigns final grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Essay for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_essay = \"\"\"The Impact of Artificial Intelligence on Education\n",
    "\n",
    "Artificial intelligence is revolutionizing education in many ways. AI can personalize learning \n",
    "by adapting to each students pace and style. For example, intelligent tutoring systems can \n",
    "identify where a student struggles and provide targeted practice.\n",
    "\n",
    "However, their are concerns about AI in education. Some worry that students might become to \n",
    "dependent on AI tools and not develop critical thinking skills. Others point out that AI \n",
    "systems can perpetuate biases if there trained on biased data.\n",
    "\n",
    "Despite these challenges, the benefits outweigh the risks. AI can help teachers by automating \n",
    "administrative tasks, allowing them to focus on actual teaching. It can also make education \n",
    "more accessible to students in remote areas who otherwise wouldn't have access to quality \n",
    "instruction.\n",
    "\n",
    "In conclusion, AI has the potential to transform education for the better, but we must \n",
    "implement it thoughtfully and address the ethical concerns. The future of education will \n",
    "likely involve a partnership between human teachers and AI systems, combining the best of both.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample essay loaded!\")\n",
    "print(f\"Length: {len(sample_essay.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Specialized Agent Tools\n",
    "\n",
    "First, let's create tools that represent specialized capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarCheckTool(BaseTool):\n",
    "    \"\"\"Tool for checking grammar and writing quality\"\"\"\n",
    "    \n",
    "    name = \"check_grammar\"\n",
    "    description = (\n",
    "        \"Analyzes text for grammar, spelling, and clarity issues. \"\n",
    "        \"Input: essay text. Returns: detailed grammar feedback.\"\n",
    "    )\n",
    "    \n",
    "    def use(self, tool_input: str) -> str:\n",
    "        \"\"\"Simulate grammar checking\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Simple checks (real systems use NLP libraries like LanguageTool)\n",
    "        if \"their are\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'their are' - should be 'there are' (wrong homophone)\")\n",
    "        if \"to dependent\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'to dependent' - should be 'too dependent' (wrong homophone)\")\n",
    "        if \"students pace\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'students pace' - missing apostrophe: 'student's pace'\")\n",
    "        if \"there trained\" in tool_input.lower():\n",
    "            issues.append(\"- Found 'there trained' - should be 'they're trained' (contraction needed)\")\n",
    "        \n",
    "        if issues:\n",
    "            return \"Grammar Issues Found:\\n\" + \"\\n\".join(issues) + \"\\n\\nOverall: Multiple homophone and apostrophe errors detected.\"\n",
    "        else:\n",
    "            return \"No major grammar issues detected. Writing is clear and correct.\"\n",
    "\n",
    "\n",
    "class ContentAnalysisTool(BaseTool):\n",
    "    \"\"\"Tool for analyzing argument quality and evidence\"\"\"\n",
    "    \n",
    "    name = \"analyze_content\"\n",
    "    description = (\n",
    "        \"Evaluates the quality of arguments and evidence in an essay. \"\n",
    "        \"Input: essay text. Returns: content quality assessment.\"\n",
    "    )\n",
    "    \n",
    "    def use(self, tool_input: str) -> str:\n",
    "        \"\"\"Simulate content analysis\"\"\"\n",
    "        text_lower = tool_input.lower()\n",
    "        \n",
    "        # Count evidence markers\n",
    "        evidence_markers = [\"for example\", \"research shows\", \"studies indicate\", \"data suggests\"]\n",
    "        evidence_count = sum(1 for marker in evidence_markers if marker in text_lower)\n",
    "        \n",
    "        # Check structure\n",
    "        has_intro = \"introduction\" in text_lower or tool_input.strip().startswith((\"Artificial\", \"The\", \"In\"))\n",
    "        has_conclusion = \"conclusion\" in text_lower or \"in summary\" in text_lower\n",
    "        \n",
    "        # Check for counterarguments\n",
    "        has_counterargument = any(word in text_lower for word in [\"however\", \"although\", \"despite\", \"concern\"])\n",
    "        \n",
    "        feedback = \"Content Analysis Results:\\n\\n\"\n",
    "        feedback += f\"Structure:\\n\"\n",
    "        feedback += f\"  - Clear introduction: {'Yes' if has_intro else 'No'}\\n\"\n",
    "        feedback += f\"  - Clear conclusion: {'Yes' if has_conclusion else 'No'}\\n\\n\"\n",
    "        feedback += f\"Argumentation:\\n\"\n",
    "        feedback += f\"  - Evidence examples provided: {evidence_count}\\n\"\n",
    "        feedback += f\"  - Addresses counterarguments: {'Yes' if has_counterargument else 'No'}\\n\\n\"\n",
    "        \n",
    "        if evidence_count < 2:\n",
    "            feedback += \"Suggestion: Add more specific examples and evidence to support claims. \"\n",
    "            feedback += \"Consider citing research studies or real-world data.\\n\"\n",
    "        if has_counterargument:\n",
    "            feedback += \"Strength: Essay acknowledges opposing views, showing critical thinking.\\n\"\n",
    "        \n",
    "        return feedback\n",
    "\n",
    "\n",
    "# Create the tools\n",
    "grammar_tool = GrammarCheckTool()\n",
    "content_tool = ContentAnalysisTool()\n",
    "\n",
    "print(\"âœ“ Specialized tools created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Specialized Agents\n",
    "\n",
    "Now we'll create three agents with different roles and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Grammar Specialist\n",
    "grammar_llm = HuggingFaceAdapter(model_name=\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "grammar_registry = ToolRegistry()\n",
    "grammar_registry.register(grammar_tool)\n",
    "grammar_executor = ToolExecutor(grammar_registry)\n",
    "grammar_memory = WorkingMemory()\n",
    "grammar_planner = SimpleReActPlanner(grammar_llm, grammar_registry)\n",
    "\n",
    "grammar_planner.prompt_builder.role_definition = RoleDefinition(\n",
    "    \"You are a Grammar Specialist and writing instructor. \"\n",
    "    \"Your job is to evaluate the grammar, spelling, and writing clarity of essays. \"\n",
    "    \"Use the check_grammar tool to analyze the text, then provide specific feedback. \"\n",
    "    \"Rate grammar quality on a scale of 1-10 and explain your rating. \"\n",
    "    \"Be constructive and identify both strengths and areas for improvement.\"\n",
    ")\n",
    "\n",
    "grammar_agent = SimpleAgent(\n",
    "    llm=grammar_llm,\n",
    "    planner=grammar_planner,\n",
    "    tool_executor=grammar_executor,\n",
    "    memory=grammar_memory,\n",
    "    max_steps=5\n",
    ")\n",
    "\n",
    "print(\"âœ“ Grammar Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 2: Content Specialist\n",
    "content_llm = HuggingFaceAdapter(model_name=\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "content_registry = ToolRegistry()\n",
    "content_registry.register(content_tool)\n",
    "content_executor = ToolExecutor(content_registry)\n",
    "content_memory = WorkingMemory()\n",
    "content_planner = SimpleReActPlanner(content_llm, content_registry)\n",
    "\n",
    "content_planner.prompt_builder.role_definition = RoleDefinition(\n",
    "    \"You are a Content Specialist and critical thinking instructor. \"\n",
    "    \"Your job is to evaluate the quality of arguments, evidence, and logical structure in essays. \"\n",
    "    \"Use the analyze_content tool to assess the text, then provide feedback on argument strength. \"\n",
    "    \"Rate content quality on a scale of 1-10 and explain your rating. \"\n",
    "    \"Evaluate: thesis clarity, evidence quality, counterargument consideration, and logical flow.\"\n",
    ")\n",
    "\n",
    "content_agent = SimpleAgent(\n",
    "    llm=content_llm,\n",
    "    planner=content_planner,\n",
    "    tool_executor=content_executor,\n",
    "    memory=content_memory,\n",
    "    max_steps=5\n",
    ")\n",
    "\n",
    "print(\"âœ“ Content Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3: Coordinator (no special tools, just synthesis)\n",
    "coordinator_llm = HuggingFaceAdapter(model_name=\"dolphin3-qwen25-3b\", auth_token=token)\n",
    "coordinator_registry = ToolRegistry()  # No tools needed\n",
    "coordinator_executor = ToolExecutor(coordinator_registry)\n",
    "coordinator_memory = WorkingMemory()\n",
    "coordinator_planner = SimpleReActPlanner(coordinator_llm, coordinator_registry)\n",
    "\n",
    "coordinator_planner.prompt_builder.role_definition = RoleDefinition(\n",
    "    \"You are the Lead Evaluator and Coordinator. \"\n",
    "    \"Your job is to synthesize feedback from the Grammar and Content specialists. \"\n",
    "    \"Review their assessments carefully, then provide: \"\n",
    "    \"1) An overall grade (A, B, C, D, or F) \"\n",
    "    \"2) A summary of key strengths \"\n",
    "    \"3) The top 3 areas for improvement \"\n",
    "    \"4) One actionable next step for the student \"\n",
    "    \"Be fair, balanced, and constructive in your evaluation.\"\n",
    ")\n",
    "\n",
    "coordinator_agent = SimpleAgent(\n",
    "    llm=coordinator_llm,\n",
    "    planner=coordinator_planner,\n",
    "    tool_executor=coordinator_executor,\n",
    "    memory=coordinator_memory,\n",
    "    max_steps=5\n",
    ")\n",
    "\n",
    "print(\"âœ“ Coordinator Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Committee\n",
    "\n",
    "Now let's coordinate the agents to evaluate our essay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_essay_with_committee(essay_text):\n",
    "    \"\"\"Coordinate multiple agents to evaluate an essay\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\" \" * 20 + \"ESSAY EVALUATION COMMITTEE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Grammar Agent evaluates\n",
    "    print(\"\\n[STEP 1: GRAMMAR SPECIALIST ANALYZING...]\\n\")\n",
    "    grammar_feedback = await grammar_agent.arun(\n",
    "        f\"Please evaluate the grammar and writing quality of this essay:\\n\\n{essay_text}\"\n",
    "    )\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(\"GRAMMAR SPECIALIST REPORT:\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(grammar_feedback)\n",
    "    \n",
    "    # Step 2: Content Agent evaluates\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"\\n[STEP 2: CONTENT SPECIALIST ANALYZING...]\\n\")\n",
    "    content_feedback = await content_agent.arun(\n",
    "        f\"Please evaluate the content quality and argumentation of this essay:\\n\\n{essay_text}\"\n",
    "    )\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(\"CONTENT SPECIALIST REPORT:\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(content_feedback)\n",
    "    \n",
    "    # Step 3: Coordinator synthesizes\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"\\n[STEP 3: LEAD EVALUATOR SYNTHESIZING...]\\n\")\n",
    "    \n",
    "    coordinator_prompt = f\"\"\"\n",
    "Please review the following specialist reports and provide a final evaluation:\n",
    "\n",
    "GRAMMAR SPECIALIST REPORT:\n",
    "{grammar_feedback}\n",
    "\n",
    "CONTENT SPECIALIST REPORT:\n",
    "{content_feedback}\n",
    "\n",
    "Provide:\n",
    "1. Overall Grade (A, B, C, D, or F)\n",
    "2. Summary of Strengths (2-3 points)\n",
    "3. Key Areas for Improvement (top 3)\n",
    "4. One Actionable Next Step for the student\n",
    "\"\"\"\n",
    "    \n",
    "    final_evaluation = await coordinator_agent.arun(coordinator_prompt)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" \" * 25 + \"FINAL EVALUATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(final_evaluation)\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the committee evaluation!\n",
    "await evaluate_essay_with_committee(sample_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Question 3\n",
    "\n",
    "**What advantages does the committee approach have over a single agent?**\n",
    "\n",
    "Think about:\n",
    "- **Specialization**: Each agent focuses on what it does best\n",
    "- **Division of labor**: Complex task broken into manageable pieces\n",
    "- **Checks and balances**: Multiple perspectives reduce bias\n",
    "- **Transparency**: You can see each agent's reasoning\n",
    "- **Modularity**: Easy to add/remove/improve individual specialists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Multi-Agent Patterns\n",
    "\n",
    "### Common Committee Structures:\n",
    "\n",
    "**1. Sequential Pipeline** (what we just built):\n",
    "```\n",
    "Essay â†’ Grammar Agent â†’ Content Agent â†’ Coordinator â†’ Final Grade\n",
    "```\n",
    "\n",
    "**2. Parallel Processing**:\n",
    "```\n",
    "               â”Œâ”€â†’ Grammar Agent â”€â”\n",
    "    Essay â”€â”€â”€â”€â”¤                   â”œâ”€â”€â†’ Coordinator â†’ Result\n",
    "               â””â”€â†’ Content Agent â”€â”˜\n",
    "```\n",
    "*Both specialists work simultaneously, then coordinator combines*\n",
    "\n",
    "**3. Hierarchical Structure**:\n",
    "```\n",
    "    Manager Agent\n",
    "         |\n",
    "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  Worker 1  Worker 2  Worker 3\n",
    "```\n",
    "*Manager delegates tasks and coordinates*\n",
    "\n",
    "**4. Debate/Consensus**:\n",
    "```\n",
    "Agent A â†â†’ Agent B â†â†’ Agent C\n",
    "     â†“         â†“         â†“\n",
    "        Consensus Result\n",
    "```\n",
    "*Agents discuss and reach agreement*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Test the committee on your own essay or writing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_essay = \"\"\"\n",
    "Paste your essay here!\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to test:\n",
    "# await evaluate_essay_with_committee(your_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights: Multi-Agent Systems\n",
    "\n",
    "### Why Committees Beat Single Agents:\n",
    "\n",
    "**Specialization**:\n",
    "- Each agent masters one skill\n",
    "- Like human experts, focused agents perform better\n",
    "- Reduces cognitive load per agent\n",
    "\n",
    "**Scalability**:\n",
    "- Add new specialists as needed\n",
    "- Agents can work in parallel for speed\n",
    "- Modular architecture is easier to maintain\n",
    "\n",
    "**Reliability**:\n",
    "- Multiple perspectives catch more issues\n",
    "- One agent's weakness is another's strength\n",
    "- Cross-validation reduces errors\n",
    "\n",
    "**Transparency**:\n",
    "- See each agent's reasoning\n",
    "- Easier to debug and improve\n",
    "- Clear audit trail\n",
    "\n",
    "### When to Use Committees:\n",
    "- âœ… Complex tasks requiring multiple skills\n",
    "- âœ… Quality-critical applications (grading, review, analysis)\n",
    "- âœ… Tasks that benefit from different perspectives\n",
    "- âœ… When you need explainability and transparency\n",
    "\n",
    "### When NOT to Use Committees:\n",
    "- âŒ Simple queries (overkill and slower)\n",
    "- âŒ Extremely time-sensitive tasks (coordination adds latency)\n",
    "- âŒ Tasks requiring deep context sharing (single agent is easier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Combining Grounding + Committees\n",
    "\n",
    "The most powerful systems combine BOTH techniques:\n",
    "\n",
    "```\n",
    "Example: Fact-Checked Essay Grader\n",
    "\n",
    "Grammar Agent (specialist)\n",
    "     â†“\n",
    "Content Agent (specialist)\n",
    "     â†“\n",
    "Fact-Checker Agent (grounded on knowledge base via RAG)\n",
    "     â†“\n",
    "Coordinator (synthesizes all feedback)\n",
    "     â†“\n",
    "Final Report with verified facts\n",
    "```\n",
    "\n",
    "Each agent is:\n",
    "- **Grounded** (has access to relevant knowledge via RAG)\n",
    "- **Specialized** (focuses on one task)\n",
    "- **Coordinated** (works with other agents)\n",
    "\n",
    "### Real-World Application Ideas:\n",
    "1. **Research Assistant**: Search agent (RAG on papers) â†’ Summarizer â†’ Synthesizer\n",
    "2. **Code Review**: Linter â†’ Security Checker (grounded on CVE database) â†’ Style Reviewer\n",
    "3. **Customer Support**: Intent Classifier â†’ Knowledge Base Agent (RAG) â†’ Response Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "### 1. How does grounding solve the hallucination problem?\n",
    "### 2. When would you use RAG vs. fine-tuning a model?\n",
    "### 3. What are the tradeoffs between single agents and committees?\n",
    "### 4. Can you think of a real-world task that would benefit from BOTH grounding AND multiple agents?\n",
    "### 5. How might you apply these techniques to your final project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've learned advanced agentic AI techniques:\n",
    "\n",
    "### Grounding Foundation Models:\n",
    "- âœ“ Why LLMs hallucinate\n",
    "- âœ“ How RAG works (Retrieve â†’ Augment â†’ Generate)\n",
    "- âœ“ Using DocumentProcessor to load documents\n",
    "- âœ“ Creating SimpleRetriever for semantic search\n",
    "- âœ“ Building agents with KnowledgeBaseQueryTool\n",
    "- âœ“ Verifiable, source-backed responses\n",
    "\n",
    "### Multi-Agent Committees:\n",
    "- âœ“ Agent specialization and roles\n",
    "- âœ“ Coordinating multiple agents\n",
    "- âœ“ Sequential and parallel workflows\n",
    "- âœ“ Synthesizing diverse feedback\n",
    "- âœ“ When to use committees vs. single agents\n",
    "\n",
    "### What's Next?\n",
    "- Experiment with different committee structures\n",
    "- Build agents grounded on YOUR data (PDFs, docs, websites)\n",
    "- Combine techniques for your final projects\n",
    "- Explore advanced patterns (debate, voting, hierarchies)\n",
    "- Build production-ready multi-agent systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Challenges (Optional)\n",
    "\n",
    "If you want to explore further, try these:\n",
    "\n",
    "### Challenge 1: Expand the Knowledge Base\n",
    "Add more UCCS documents (parking info, dining services, athletics) and test the grounded agent.\n",
    "\n",
    "### Challenge 2: Add a Third Specialist\n",
    "Create a \"Citation Checker\" or \"Fact Verifier\" agent that uses RAG to verify claims in essays.\n",
    "\n",
    "### Challenge 3: Parallel Evaluation\n",
    "Modify the committee to run grammar and content agents in parallel using `asyncio.gather()`:\n",
    "```python\n",
    "grammar_task, content_task = await asyncio.gather(\n",
    "    grammar_agent.arun(prompt),\n",
    "    content_agent.arun(prompt)\n",
    ")\n",
    "```\n",
    "\n",
    "### Challenge 4: Grounded Fact-Checker\n",
    "Add a fourth agent that uses the knowledge base to verify factual claims in essays.\n",
    "\n",
    "### Challenge 5: Your Own Multi-Agent System\n",
    "Design a committee for a task relevant to your final project. Consider:\n",
    "- What specialists are needed?\n",
    "- Which agents need RAG access?\n",
    "- Sequential or parallel coordination?\n",
    "- How to synthesize results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Math Demo)",
   "language": "python",
   "name": "llm-math-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
